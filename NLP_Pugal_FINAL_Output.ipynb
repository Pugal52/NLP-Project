{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
      "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwNns9FzDV8r",
        "outputId": "2897cbf1-a2dd-405e-e983-f8c32a635e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m655.4/981.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=eb618b8cc4faf24346d43e006268231953c1f583e537eea2dd4bfb8cf26ad7fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRINTS ONLY COMMENT AND SENTIMENT"
      ],
      "metadata": {
        "id": "OwqGWODUThme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Initialize YouTube API\n",
        "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
        "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "\n",
        "# Initialize models\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def extract_video_id(url):\n",
        "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
        "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def get_comments(video_id):\n",
        "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "    while request:\n",
        "        response = request.execute()\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "        request = youtube.commentThreads().list_next(request, response)\n",
        "    return comments\n",
        "\n",
        "def detect_language(comment):\n",
        "    \"\"\"Detect the language of a comment.\"\"\"\n",
        "    try:\n",
        "        return detect(comment)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def clean_comment(comment):\n",
        "    \"\"\"Clean the comment by removing special characters and links.\"\"\"\n",
        "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
        "    comment = re.sub(r\"[^\\w\\s]\", \"\", comment)  # Remove special characters\n",
        "    return comment.strip().lower()\n",
        "\n",
        "def analyze_sentiment(comment):\n",
        "    \"\"\"Analyze the sentiment of a comment.\"\"\"\n",
        "    result = sentiment_pipeline(comment)[0]\n",
        "    return result[\"label\"], result[\"score\"]\n",
        "\n",
        "def identify_issue_comments(comments):\n",
        "    \"\"\"Identify comments that mention issues.\"\"\"\n",
        "    issue_keywords = [\"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\"]\n",
        "    issue_comments = []\n",
        "    for comment in comments:\n",
        "        if any(keyword in comment for keyword in issue_keywords):\n",
        "            issue_comments.append(comment)\n",
        "    return issue_comments\n",
        "\n",
        "def extract_timestamps(comment):\n",
        "    \"\"\"Extract timestamps from a comment.\"\"\"\n",
        "    return re.findall(r\"\\b\\d{1,2}:\\d{2}\\b\", comment)\n",
        "\n",
        "def recommend_fixes(issue_comments):\n",
        "    \"\"\"Provide recommendations based on identified issues.\"\"\"\n",
        "    recommendations = []\n",
        "    for comment in issue_comments:\n",
        "        if \"audio\" in comment or \"sound\" in comment:\n",
        "            recommendations.append(\"Check the audio levels and ensure there are no issues with the sound quality.\")\n",
        "        elif \"video\" in comment or \"visual\" in comment:\n",
        "            recommendations.append(\"Review the video quality and ensure there are no visual glitches.\")\n",
        "        elif \"sync\" in comment:\n",
        "            recommendations.append(\"Ensure that the audio and video are properly synchronized.\")\n",
        "        else:\n",
        "            recommendations.append(\"Review the mentioned issue and consider appropriate fixes.\")\n",
        "    return recommendations\n",
        "\n",
        "def main():\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL.\")\n",
        "        return\n",
        "\n",
        "    print(\"Fetching comments...\")\n",
        "    comments = get_comments(video_id)\n",
        "    if not comments:\n",
        "        print(\"No comments found.\")\n",
        "        return\n",
        "\n",
        "    print(\"Processing comments...\")\n",
        "    clean_comments = [clean_comment(c) for c in comments]\n",
        "    issue_comments = identify_issue_comments(clean_comments)\n",
        "\n",
        "    if not issue_comments:\n",
        "        print(\"No issue-related comments found.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nIdentified Issues and Recommendations:\")\n",
        "    for comment in issue_comments:\n",
        "        timestamps = extract_timestamps(comment)\n",
        "        sentiment, score = analyze_sentiment(comment)\n",
        "        recommendation = recommend_fixes([comment])[0]\n",
        "        print(f\"\\nComment: {comment}\")\n",
        "        if timestamps:\n",
        "            print(f\"Timestamps: {', '.join(timestamps)}\")\n",
        "        print(f\"Sentiment: {sentiment} (Confidence: {score:.2f})\")\n",
        "        print(f\"Recommendation: {recommendation}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c61ebdeeb404464281dafe00dbb2d9b8",
            "16c80048e7b84d6fb5d6a1ad7a542221",
            "b1bb0254287144d996098bb8a0b39055",
            "21725b42d4094f95a53abf90706ebd0f",
            "2c9bbd6f4a0e4d92903c0eb45df3a4ef",
            "5611142dde904579a69b08fe635cc8d6",
            "7c6c6d9d4d2f4ceb86fa714e94370c39",
            "974ba8ee5a7f433b9a4ee5180cea0201",
            "0c05d3dcdb544c7f98d9aa76701c9100",
            "b78d2f4564f2400c8540b5b3bd79f59b",
            "765534b0671242ebb14f9cd11a1ee98a",
            "d719a2e1930e4e3eb04e8cabb19e66d5",
            "5ae6f262fe3941f0b6e61408fd416cd4",
            "0aae521063694c639b6c81e66247581c",
            "a9c7ac3ca4ae4029be7ba5c58a35bb65",
            "222a02bf45d342789d30d5d9184dd6d4",
            "825b866fbdde43b5a5d4a6310d74aa9f",
            "fffe72f12d36498cbbe3786a8c2beb24",
            "27f4e3a5a5934d6a94cbb86465211b5a",
            "62c15a87931745e6a9eb8927aed8dd35",
            "270577c2be4d48f980b40deef46a2537",
            "d6759fb28af04cc4b99a6ab10e798dab",
            "9670bf7c245241edb9dc0bcb0ac924a4",
            "25a5e40291614bbeb5a5c9f1274234c5",
            "91d47ddba4994cfa9d6734e88997c2cb",
            "46432018c7c94ce4a6b643793689c355",
            "d0c1565a151b4838bed321a8177c672e",
            "3258c2fc2f454c62bf9571a8dad4a716",
            "ac253f90b5314cba975b9e81ee8db5bf",
            "9e51067711264eb8a4fced64d0b78749",
            "8a8813761f544b1c9491b494865e0d5c",
            "1a1eeffbb75a4b658f8cbbb7763f9c0a",
            "5c2f9f44785a417697539e6ca3b471ea",
            "9ceb16e83f8e4d31b6f2058e73204f83",
            "cafceb93587e4946ba9624ac9c00b135",
            "2eb80e56f9cc49fa8a3b1df031d0de87",
            "eafddacb81714af0a81f1d2e22a1d812",
            "e45a6ffc4c454f2c926c4779b306d578",
            "20f2e5ee3b234c13a3c9924abf338ab0",
            "41a310138bb84c18bcaf94880dfec7b9",
            "74f03eff69764f19a7ac7d36ad05f1c0",
            "1175ac16a0b14d348587be1015f18c0c",
            "46681183fc2d473995bb2530f0b538bf",
            "b7c81a985ac4467394d0e33520bf9a82",
            "d51f6beae0e948218e0d3b3394623494",
            "8168e82e54624f718a5c7a11645b0eaa",
            "3412441d2c8642a4a1e1433593f0e139",
            "667cfabbd7c34247ac48f44afb9057de",
            "5c299f5da0644f259f234d120bae3453",
            "319d2f43116d404e8d39d934708d7645",
            "254d1b17889d4b7c981a825f9715bf51",
            "7c2a5e39232d4326bc3ddd7f447ecd6f",
            "5335136c2e72431ba24ca73bde8ba526",
            "585bec5fea394e5db0c1049a8d2a7fd7",
            "ae300cc7947a42dd98008ea22e70836b",
            "2c328af20eba4124bde084634c3cb6fe",
            "b7826286466248358c52fe09c44b7b9a",
            "c7c4aeb7caa3416c805f7bec957e90a3",
            "44fe45f7b49d446dbbdd1d3effa0cd70",
            "d9e67637d6434ae38e92642de74129e2",
            "713e8dbcea1e40199ebde7bc628ae68b",
            "f0ddd2dccd464f72a3de01c0b2972f2c",
            "52adbb9ff8404340932b0c20b5b3591a",
            "f27cffd82b914e4b9d800bb99ddd1ca1",
            "683c80aa5a324701822fb4a9bdf385e4",
            "e1fb01b629d742239926534ef7f92b2e",
            "cd228ea3c2c44443887834947724f233",
            "4aa580dcf6754f81a16f77f333eba9b7",
            "2406bbb7c63c4ad7aa4af35b293d2087",
            "c9c94fd723d44c688b0dba438a460a57",
            "9193a34fe1734603a5825f0349a7f989",
            "52cbbb0c245e4663b13a493454f7ff3a",
            "2fa9c6cd3df24369b0a637c298f15234",
            "0f67a14f6a3e4468b31e30c86e939df1",
            "ecb196d33f174996a741b09c91ac93b2",
            "05467f67d1104187b00e42c688b2d879",
            "97e5ab38ca994c3abea844789021277f",
            "43dd5a64e50547c59fe7628c533b1f92",
            "b5121d61693a423fb4cd6e97f9f33c89",
            "047c4d85f06e4013acd42a19716379af",
            "d643fa560370446bb7277729c6b752fc",
            "e28c79d324074eec8acb123612940f56",
            "d2b0f54008414621a771c869a5f07a8e",
            "2e876b37896e4c47aacbf954e0018e2c",
            "f3ab39e888804432a647bfa89af9894a",
            "660824ea3d2944eca6308515e724bf41",
            "1703d7cb2f164e618ab5116128277fb3",
            "2286e3cba5cb461c9d3749697eb038d3",
            "558e5eea46314777996392b047912817",
            "760a65c1791545879e6722ba43db157a",
            "0fc5d3bee73342cfae111b1e4c416a17",
            "bef3799053dd4f53a9857c4cbcdd867c",
            "d60aed81383e4887ac8ffb7ba285ba1b",
            "6a983c78749b407b9107fa13836d2818",
            "ff1f191e09da42b5b24fdd154bb5869f",
            "941e90bc261243ad8a5100aaca2cd884",
            "b16d12a95b0d40c98a53a23deff1f1e0",
            "7a443c38166c488688ab189a79707362",
            "97e0748205124fb58eec831894994055",
            "2c706910387b4b89b0c8d057040d5690",
            "9c5ecd4001cf41cfa3f9e584ea1b5620",
            "5cb9954b404d4536bceeef3a0a8f1b4f",
            "b0236c2234b846deacf933c5d675cd13",
            "fb011762ac60460f912c0066204d1456",
            "2ecf870ac4a044d2a91c55df66e65cca",
            "6a1194f691004d3e9142acb5d2c26321",
            "78da097fc4c343ab8ad447ac0121afe8",
            "787191bb73af49bb8a07621f8977cf20",
            "67a5f92bd6f846089a6776a4e0d53058",
            "f2e10da811704c6680f5d961bb68b5e3",
            "806ebff8a95e46258133ab019cd97fb7",
            "d752c9586e3b422ca0c1a288b5709dd5",
            "aeccdd14d4b44394ac30be14140f2417",
            "cc4c45d65dfc4b4cae69f19cd49bdeb2",
            "d16c66c72e51475fbb76b5c6d8b2f22b",
            "7d6dcb776bad47bea6c2a92c4e0baedf",
            "1bae59f02902473a839d9d6b943af32c",
            "101432388a924dbd993fd8db35a6e6dd",
            "b19ea5ec2f6443c7a1d64f05fbb83451",
            "08df0841810b497ba66d7c202d8ca303",
            "a986e1df688344d5a0b3bb626fb6234b",
            "43ab8d9dafd040fa9d48272fbce5d27a",
            "1d3c1477819c46e1b6d47a4e778cebd9",
            "d9ee20a20100429db6a29a0503f47915",
            "9ad97a24d47346b7bc787834265b8044",
            "67875d390bca4d8899edcf9239393892",
            "601156eee4f74b569268670a7d4eda1d",
            "1bfa6b552ba64f80898f5794d656b367",
            "b345b84ef53d43b3bbe38d8d2d3977a9",
            "58d2a605ef234f69804009f275f96516",
            "377d006f41024a779d55dccb1e2e6e99",
            "2e3ec7a60e2045cea2a892ae8f5f62eb",
            "4556b4c181294c34991ccbb7b52c45ee",
            "e67fa1c6bea542acb14f8583f2f32c65",
            "c120ccb272a04e52b8254d74404c2aac",
            "df3f911366c74e11b5a8f403e3f2d135",
            "2d6ef320ca734f598a15a7886818239c",
            "a144d90b94314625869ff44892f6b60e",
            "cc69f3c71ad443c1aad9b9d16b372e28",
            "4a79f681ee4b4b239d618c3fb20b6cab",
            "a0eb4d27073645b0aa54f3ee04625836",
            "17fa1bb829f34b9caf350916525511ad",
            "5c25bf8fa3cc4a68a8bb2c1160d71319",
            "5f3b7cb0be10493f90c675b95b2eafb5",
            "4192c37ba1314ed9aff9ab473847d0c0",
            "bf567f5de68c4de6b9687dc56843b1a2",
            "4175c4f354b6479d90c4d550f2d10f4c",
            "1a90bdec96cf417da89b889d5f4c113a",
            "b950c58b8dd24f6e9b7d7aa263d4920a",
            "d5953c73558b487aa8c388a5e5378491",
            "ee03c23597d640de8182807199d10dc3",
            "56ca4c9da1584e35a873cae02ce5d03f",
            "df6d9701a78544bdbb1ba9e8e06eb668",
            "daf802101d1d40eeab45b5e4ab666cd9",
            "280a1e817ae047998586657f97f51ecb",
            "43124260d1f2477e979b1f404f1684a2",
            "77a72b5a10504a06aeef7139fc5eec91",
            "3d8cdc04378b4307921ae8780c6c8cf0",
            "849d05194de24d5494984d6c42037f91",
            "9cbad7099943443f936deb40a336aab2",
            "0249bd13e2f24acba89960ac689d6974",
            "ddf71f2b950744d4b2fbd79c051c9cf4",
            "49bba91539374a2ba77f8dc43196c465",
            "749bd8dcc3d44dacabf2132941d256b7",
            "64d5c9bc630f4c2392cdabd486816df6"
          ]
        },
        "id": "eW7IQgkXHdZx",
        "outputId": "7b07c985-a5a5-4751-e98b-c924489031c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c61ebdeeb404464281dafe00dbb2d9b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d719a2e1930e4e3eb04e8cabb19e66d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9670bf7c245241edb9dc0bcb0ac924a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ceb16e83f8e4d31b6f2058e73204f83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d51f6beae0e948218e0d3b3394623494"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c328af20eba4124bde084634c3cb6fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd228ea3c2c44443887834947724f233"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43dd5a64e50547c59fe7628c533b1f92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "558e5eea46314777996392b047912817"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c706910387b4b89b0c8d057040d5690"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "806ebff8a95e46258133ab019cd97fb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43ab8d9dafd040fa9d48272fbce5d27a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4556b4c181294c34991ccbb7b52c45ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f3b7cb0be10493f90c675b95b2eafb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "280a1e817ae047998586657f97f51ecb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://youtu.be/PqVvrF7pwDQ?si=WCpS32eGnwUMk_xz\n",
            "Fetching comments...\n",
            "Processing comments...\n",
            "\n",
            "Identified Issues and Recommendations:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comment: cen you find tha mistake\r\n",
            "\r\n",
            "123426789\r\n",
            "\r\n",
            "the clue \r\n",
            "\r\n",
            "there or three mistake  \r\n",
            "\r\n",
            "please any one like my comment\r\n",
            "\r\n",
            "\r\n",
            "extremely hard level \r\n",
            "\r\n",
            "any one comment\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: abhay  bro completely stloe our hearts with the enthusiastic scenes and creationhis creative ideas never failed to entertain usthere will be no errors in his creation\n",
            "tons of love abhaydo great\n",
            "Sentiment: POSITIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 039 laye framing issue iruku nenaikrennnn\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: finally all the glitches and mistakes has been fixed added extra scenes effects  transitions the ending with disappearing effect added more meaningfull than before kudos to the team\n",
            "Sentiment: POSITIVE (Confidence: 0.86)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: came to see the glitch at 330\n",
            "Sentiment: NEGATIVE (Confidence: 0.93)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: his music has one problem once you hear it you become addicted\n",
            "Sentiment: NEGATIVE (Confidence: 0.98)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: i have serious problem with this song i couldnt take it out from my brain people said its slow addiction but for me the song kept lingering in my mind throughout the day from the day i first watched it \n",
            " apart from ille ille from min 257308 is my most fav part i couldnt resist in watching it in repeat \n",
            "sai bro a genuine question to u in all ur music videos there would be a part where ull stare at the heroine is there any specific reason to it\n",
            "Sentiment: POSITIVE (Confidence: 0.85)\n",
            "Recommendation: Review the video quality and ensure there are no visual glitches.\n",
            "\n",
            "Comment: ive watched this song so many times that i couldnt help but notice the changes not sure if im the first one to point it out but heres what i observed\n",
            "\n",
            "1 the entire video has been reuploadedwonder how many of you noticed\n",
            "\n",
            "2 the image of meenakshi at 029 has been replaced\n",
            "\n",
            "3 some shake effects have been added here and there though im not entirely sure\n",
            "\n",
            "4 i noticed some effect changes at 257 again not completely sure\n",
            "\n",
            "5 the main reason for the reupload seems to be the transition at 330 where abhay walks into the light in the earlier version there was an editors error where a timestamp from the footage appeared for a few milliseconds during the transition this has been corrected in the new version\n",
            "\n",
            "6 a significant change at 404 in the previous version sai placed his hands on meenakshis face before the cut in this edit meenakshi disintegrates into dust before transitioning to the next scene\n",
            "\n",
            "that being said hats off to sai abhyankkar for delivering an absolute banger despite these minor edits this song is truly a masterpiece fan of your work da thambi\n",
            "Sentiment: NEGATIVE (Confidence: 0.83)\n",
            "Recommendation: Review the video quality and ensure there are no visual glitches.\n",
            "\n",
            "Comment: glitch theda vantha me\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 bug\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: frst time suthama pudikala 4 murai keten apoyum pudikala headphonela keten konjam manage panlam thonuchu after 10th time i am in loop mode i think problem is choreography and sai expression less face and dance moves in case some other good dancer irundhaa superaa irundhurkum nalaadhanya iruku  kekalam konjanaaliku\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 331 mistake editing\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 0330 mistake\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: technical glitch 330\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: helo there is error in the footage at 331 the end the shot footage there was an video file name 41220fbmxf in the left bottom corner  in the right bottom corner src tc 12\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the video quality and ensure there are no visual glitches.\n",
            "\n",
            "Comment: 331 found a glitch\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 331 glitch\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 bro editing mistake\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 331 glitch transtion\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: theres a glitch at 331\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 editing mistake \n",
            "yeah china karan yeadhu yeadho kandu pudikaran naai yeadha kandu pudichirku paaru\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 glitch at left bottom they forgot to remove the clip name\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: there was a edit mistake in 330any one saw\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: came to see the video glitch at 33105 sec\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the video quality and ensure there are no visual glitches.\n",
            "\n",
            "Comment: 330 try to find the glitch  ithan na kandu pidichathu\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 glitch\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 329 how many of you are here after the insta video for editing mistake\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the video quality and ensure there are no visual glitches.\n",
            "\n",
            "Comment: us senate candidates we need action\n",
            "two us senate candidates and two state representative candidates shared similar views its actually astounding and infuriating that these types of egregious human rights problems are going on in china and that we are aware of it in our country and that we are not addressing the issues said independent senate nominee and former state senator dr mike katz but we fail because of the influence of money its very concerning and we need strong leadership in this country to step forward and to deal with it immediately\n",
            "\n",
            "\n",
            "independent senate nominee and former state senator dr mike katz\n",
            "\n",
            "senator katz emphasized how the film revealed the true nature of communism and its harm on humanity its been a slow infiltration of communism mindset in this country its occurring in our academic institutions its the influence of money coming in from overseas that is teaching our students not how to critically think but what to think and that is very concerning about the future of our country but its been going on for several years he said\n",
            "\n",
            "senator katz said he has always been aware that there were human rights problems in china and he called on people to become aware of the issues and not just stand by\n",
            "\n",
            "us senate candidate eric hansen said he was shocked when he heard about the forced organ harvesting in china which is supported by the ccp its just such a tragedy and the fact that the world doesnt know about this the fact that youre drawing attention to it is very important he said\n",
            "\n",
            "\n",
            "us senate candidate eric hansen right with falun gong practitioner cheng peiming\n",
            "\n",
            "mr hansen said the abhorrent crime of organ harvesting must be ended get the word out please because its such a huge issue thats being unaddressed he stated\n",
            "Sentiment: NEGATIVE (Confidence: 0.97)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 error\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 331 what this specific time at lowest speed and see if there is any problem\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 glitch in video with an unedited frame\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the video quality and ensure there are no visual glitches.\n",
            "\n",
            "Comment: finally a song over 3 minutes these days songs ends so fast there was even a 1 minute song by gv prakash are the musicians becoming lazy making songs under 230 mins or is the scrolling culture part of the problem\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: 330 got a glitch in this part of transition and the video file number was shown in the left bottom of the video\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the video quality and ensure there are no visual glitches.\n",
            "\n",
            "Comment: editing mistake on 331 raw footage popping out for a millisec\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: saiabhyankkar you nailed it again brother  \n",
            "if anyone doesnt love this song please consult your doctor as soon as possible just tell him that you have some problem from the neck up\n",
            "Sentiment: NEGATIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: bro deviated from his pattern and made a small mistake his songs instant attraction is hook step with heroine bro tried to break this pattern by drastically reducing meenakshis screen presence and dance scenes which might make this song not a big trend than previous songs\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n",
            "\n",
            "Comment: i agree that this song is indeed a banger however the problem here is that the last two songs katchi sera and aasa kooda gained popularity because of their impressive dance routines both girls performed exceptionally well in those dances in contrast in this song youve only used meenakshi chaudhary for a few simple walks which are expected to be the 30second dances i dont think this song will achieve the same level of popularity as your previous two songs by the way those 127 and 336 sections were truly a peak moment that i thoroughly enjoyed its not easy to get so much output i really appreciate the team\n",
            "Sentiment: POSITIVE (Confidence: 0.99)\n",
            "Recommendation: Review the mentioned issue and consider appropriate fixes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NO SUMMARY AND NO TIMESTAMPS BUT WORKS"
      ],
      "metadata": {
        "id": "FUMNV3ykTP2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from googleapiclient.discovery import build\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize YouTube API\n",
        "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
        "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "\n",
        "# Initialize models\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def extract_video_id(url):\n",
        "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
        "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def get_comments(video_id):\n",
        "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "    while request:\n",
        "        response = request.execute()\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "        request = youtube.commentThreads().list_next(request, response)\n",
        "    return comments\n",
        "\n",
        "def detect_language(comment):\n",
        "    \"\"\"Detect the language of a comment.\"\"\"\n",
        "    try:\n",
        "        return detect(comment)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def clean_comment(comment):\n",
        "    \"\"\"Clean the comment by removing special characters and links.\"\"\"\n",
        "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
        "    comment = re.sub(r\"[^\\w\\s]\", \"\", comment)  # Remove special characters\n",
        "    return comment.strip().lower()\n",
        "\n",
        "def analyze_sentiment(comment):\n",
        "    \"\"\"Analyze the sentiment of a comment.\"\"\"\n",
        "    result = sentiment_pipeline(comment)[0]\n",
        "    return result[\"label\"], result[\"score\"]\n",
        "\n",
        "def identify_issue_comments(comments):\n",
        "    \"\"\"Identify comments that mention issues and have negative sentiment.\"\"\"\n",
        "    issue_keywords = [\"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\", \"not working\", \"bad\", \"broken\"]\n",
        "    issue_comments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        if any(keyword in comment for keyword in issue_keywords):\n",
        "            sentiment, score = analyze_sentiment(comment)\n",
        "            if sentiment == \"NEGATIVE\" and score > 0.75:  # Ensure it's a strong negative sentiment\n",
        "                issue_comments.append(comment)\n",
        "\n",
        "    return issue_comments\n",
        "\n",
        "def extract_timestamps(comment):\n",
        "    \"\"\"Extract timestamps from a comment.\"\"\"\n",
        "    return re.findall(r\"\\b\\d{1,2}:\\d{2}\\b\", comment)\n",
        "\n",
        "def categorize_issues(issue_comments):\n",
        "    \"\"\"Categorize detected issues and summarize timestamps.\"\"\"\n",
        "    categorized_issues = defaultdict(list)\n",
        "    timestamps_summary = defaultdict(list)\n",
        "\n",
        "    for comment in issue_comments:\n",
        "        timestamps = extract_timestamps(comment)\n",
        "\n",
        "        if \"audio\" in comment or \"sound\" in comment:\n",
        "            categorized_issues[\"Audio Issues\"].append(comment)\n",
        "            if timestamps:\n",
        "                timestamps_summary[\"Audio Issues\"].extend(timestamps)\n",
        "\n",
        "        elif \"video\" in comment or \"visual\" in comment or \"quality\" in comment:\n",
        "            categorized_issues[\"Video Issues\"].append(comment)\n",
        "            if timestamps:\n",
        "                timestamps_summary[\"Video Issues\"].extend(timestamps)\n",
        "\n",
        "        elif \"sync\" in comment or \"delay\" in comment:\n",
        "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
        "            if timestamps:\n",
        "                timestamps_summary[\"Synchronization Issues\"].extend(timestamps)\n",
        "\n",
        "        else:\n",
        "            categorized_issues[\"Other Issues\"].append(comment)\n",
        "            if timestamps:\n",
        "                timestamps_summary[\"Other Issues\"].extend(timestamps)\n",
        "\n",
        "    return categorized_issues, timestamps_summary\n",
        "\n",
        "def recommend_fixes(category):\n",
        "    \"\"\"Provide recommendations based on issue categories.\"\"\"\n",
        "    fixes = {\n",
        "        \"Audio Issues\": \"Check the audio levels, remove background noise, and verify microphone quality.\",\n",
        "        \"Video Issues\": \"Review video resolution, color grading, and any potential artifacts or glitches.\",\n",
        "        \"Synchronization Issues\": \"Ensure proper audio-video sync during editing and encoding.\",\n",
        "        \"Other Issues\": \"Review the mentioned issues and consider appropriate fixes.\"\n",
        "    }\n",
        "    return fixes.get(category, \"Investigate the issue further for a solution.\")\n",
        "\n",
        "def main():\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Fetching comments...\")\n",
        "    comments = get_comments(video_id)\n",
        "    if not comments:\n",
        "        print(\"No comments found.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Processing comments...\")\n",
        "    clean_comments = [clean_comment(c) for c in comments]\n",
        "    issue_comments = identify_issue_comments(clean_comments)\n",
        "\n",
        "    if not issue_comments:\n",
        "        print(\"‚úÖ No issue-related negative comments found.\")\n",
        "        return\n",
        "\n",
        "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
        "\n",
        "    print(\"\\nüö® Identified Issues and Recommendations üö®\")\n",
        "    for category, comments in categorized_issues.items():\n",
        "        print(f\"\\nüî∏ **{category} ({len(comments)} comments)**\")\n",
        "        for comment in comments[:5]:  # Display a few sample comments\n",
        "            print(f\"  - {comment}\")\n",
        "        print(f\"  ‚û§ Recommended Fix: {recommend_fixes(category)}\")\n",
        "\n",
        "        if category in timestamps_summary and timestamps_summary[category]:\n",
        "            print(f\"  ‚è∞ Issue Found At: {', '.join(set(timestamps_summary[category]))}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis Complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymgNQoZ0JvxW",
        "outputId": "501f1167-901c-4f2c-d0ff-bacad7bfccaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://youtu.be/PqVvrF7pwDQ?si=WCpS32eGnwUMk_xz\n",
            "üîπ Fetching comments...\n",
            "üîπ Processing comments...\n",
            "\n",
            "üö® Identified Issues and Recommendations üö®\n",
            "\n",
            "üî∏ **Other Issues (57 comments)**\n",
            "  - choreo is always good but camera angles and cuts are always bad nalla step podum pothu long poraanunga\n",
            "  - cen you find tha mistake\n",
            "\n",
            "123426789\n",
            "\n",
            "the clue \n",
            "\n",
            "there or three mistake  \n",
            "\n",
            "please any one like my comment\n",
            "\n",
            "\n",
            "extremely hard level \n",
            "\n",
            "any one comment\n",
            "  - samsung s25 edge apu nisa apple lase4 eka 16e kara venna bada\n",
            "  - i thought its bad but now addicted\n",
            "  - tamil nadu badshah\n",
            "  ‚û§ Recommended Fix: Review the mentioned issues and consider appropriate fixes.\n",
            "\n",
            "üî∏ **Video Issues (8 comments)**\n",
            "  - ive watched this song so many times that i couldnt help but notice the changes not sure if im the first one to point it out but heres what i observed\n",
            "\n",
            "1 the entire video has been reuploadedwonder how many of you noticed\n",
            "\n",
            "2 the image of meenakshi at 029 has been replaced\n",
            "\n",
            "3 some shake effects have been added here and there though im not entirely sure\n",
            "\n",
            "4 i noticed some effect changes at 257 again not completely sure\n",
            "\n",
            "5 the main reason for the reupload seems to be the transition at 330 where abhay walks into the light in the earlier version there was an editors error where a timestamp from the footage appeared for a few milliseconds during the transition this has been corrected in the new version\n",
            "\n",
            "6 a significant change at 404 in the previous version sai placed his hands on meenakshis face before the cut in this edit meenakshi disintegrates into dust before transitioning to the next scene\n",
            "\n",
            "that being said hats off to sai abhyankkar for delivering an absolute banger despite these minor edits this song is truly a masterpiece fan of your work da thambi\n",
            "  - helo there is error in the footage at 331 the end the shot footage there was an video file name 41220fbmxf in the left bottom corner  in the right bottom corner src tc 12\n",
            "  - please focus on music rather your dance after you two blockbuster music video this video is not really expected from you lyrics and the quality of the music is really bad i think you really focus on your music lyrics again i know you can do much better than this because your first two songs are really good so please dont make this type of songs\n",
            "  - came to see the video glitch at 33105 sec\n",
            "  - 329 how many of you are here after the insta video for editing mistake\n",
            "  ‚û§ Recommended Fix: Review video resolution, color grading, and any potential artifacts or glitches.\n",
            "\n",
            "‚úÖ Analysis Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORKS WITH TIMESTAMP BUT NOT IN PROPER FORMAT"
      ],
      "metadata": {
        "id": "LyPOGr3yQeXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from googleapiclient.discovery import build\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize YouTube API\n",
        "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
        "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "\n",
        "# Initialize sentiment analysis model\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def extract_video_id(url):\n",
        "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
        "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def get_comments(video_id):\n",
        "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "    while request:\n",
        "        response = request.execute()\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "        request = youtube.commentThreads().list_next(request, response)\n",
        "    return comments\n",
        "\n",
        "def detect_language(comment):\n",
        "    \"\"\"Detect the language of a comment.\"\"\"\n",
        "    try:\n",
        "        return detect(comment)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def clean_comment(comment):\n",
        "    \"\"\"Clean the comment by removing special characters and links.\"\"\"\n",
        "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
        "    comment = re.sub(r\"[^\\w\\s]\", \"\", comment)  # Remove special characters\n",
        "    return comment.strip().lower()\n",
        "\n",
        "def analyze_sentiment(comment):\n",
        "    \"\"\"Analyze the sentiment of a comment, truncating if necessary.\"\"\"\n",
        "    MAX_LENGTH = 512  # Maximum token length for the model\n",
        "\n",
        "    # Truncate the comment if it's too long\n",
        "    truncated_comment = comment[:MAX_LENGTH]\n",
        "\n",
        "    result = sentiment_pipeline(truncated_comment)[0]\n",
        "    return result[\"label\"], result[\"score\"]\n",
        "\n",
        "def extract_timestamps(comment):\n",
        "    \"\"\"Extract timestamps from a comment in various formats.\"\"\"\n",
        "    time_patterns = [\n",
        "        r\"\\b\\d{1,2}:\\d{2}(:\\d{2})?\\b\",  # MM:SS or H:MM:SS (e.g., 12:34, 1:23:45)\n",
        "        r\"\\b\\d{3,4}\\b\",  # MMSS or MSS (e.g., 1234 for 12:34, 234 for 2:34)\n",
        "        r\"\\b\\d{4,5}\\b\",  # HMMSS or HMSS (e.g., 12345 for 1:23:45, 1234 for 12:34)\n",
        "        r\"(\\d+)\\s*h\\s*(\\d+)?\\s*m?\\s*(\\d+)?\\s*s?\",  # 1h 23m 45s, 5m 10s, etc.\n",
        "    ]\n",
        "    timestamps = []\n",
        "\n",
        "    for pattern in time_patterns:\n",
        "        matches = re.findall(pattern, comment)\n",
        "        for match in matches:\n",
        "            if isinstance(match, tuple):\n",
        "                hours, minutes, seconds = match\n",
        "                time_str = f\"{hours}h {minutes}m {seconds}s\".strip()\n",
        "            else:\n",
        "                time_str = match\n",
        "            timestamps.append(time_str)\n",
        "\n",
        "    return timestamps\n",
        "\n",
        "def identify_issue_comments(comments):\n",
        "    \"\"\"Identify comments that mention issues and have negative sentiment.\"\"\"\n",
        "    issue_keywords = [\n",
        "        \"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\",\n",
        "        \"not working\", \"bad\", \"broken\", \"off\"\n",
        "    ]\n",
        "    issue_comments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        if any(keyword in comment for keyword in issue_keywords):\n",
        "            sentiment, score = analyze_sentiment(comment)\n",
        "            if sentiment == \"NEGATIVE\" and score > 0.75:  # Strong negative sentiment\n",
        "                issue_comments.append(comment)\n",
        "\n",
        "    return issue_comments\n",
        "\n",
        "def categorize_issues(issue_comments):\n",
        "    \"\"\"Categorize detected issues and store timestamps properly.\"\"\"\n",
        "    categorized_issues = defaultdict(list)\n",
        "    timestamps_summary = defaultdict(list)\n",
        "\n",
        "    for comment in issue_comments:\n",
        "        timestamps = extract_timestamps(comment)  # Extract timestamps\n",
        "\n",
        "        if any(word in comment.lower() for word in [\"audio\", \"sound\", \"voice\", \"mic\"]):\n",
        "            categorized_issues[\"Audio Issues\"].append(comment)\n",
        "            timestamps_summary[\"Audio Issues\"].extend(timestamps)\n",
        "\n",
        "        elif any(word in comment.lower() for word in [\"video\", \"visual\", \"quality\", \"blurry\", \"pixelated\"]):\n",
        "            categorized_issues[\"Video Issues\"].append(comment)\n",
        "            timestamps_summary[\"Video Issues\"].extend(timestamps)\n",
        "\n",
        "        elif any(word in comment.lower() for word in [\"sync\", \"delay\", \"out of sync\", \"lag\"]):\n",
        "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
        "            timestamps_summary[\"Synchronization Issues\"].extend(timestamps)\n",
        "\n",
        "        elif any(word in comment.lower() for word in [\"framing\", \"cropped\", \"off-center\", \"too much space\"]):\n",
        "            categorized_issues[\"Framing Issues\"].append(comment)\n",
        "            timestamps_summary[\"Framing Issues\"].extend(timestamps)\n",
        "\n",
        "        else:\n",
        "            categorized_issues[\"Other Issues\"].append(comment)\n",
        "            timestamps_summary[\"Other Issues\"].extend(timestamps)\n",
        "\n",
        "    return categorized_issues, timestamps_summary\n",
        "\n",
        "def recommend_fixes(category):\n",
        "    \"\"\"Provide recommendations based on issue categories.\"\"\"\n",
        "    fixes = {\n",
        "        \"Audio Issues\": \"Check the audio levels, remove background noise, and verify microphone quality.\",\n",
        "        \"Video Issues\": \"Review video resolution, color grading, and any potential artifacts or glitches.\",\n",
        "        \"Synchronization Issues\": \"Ensure proper audio-video sync during editing and encoding.\",\n",
        "        \"Framing Issues\": \"Adjust camera positioning to maintain proper framing and avoid unnecessary empty space.\",\n",
        "        \"Other Issues\": \"Investigate the mentioned issues and consider appropriate fixes.\"\n",
        "    }\n",
        "    return fixes.get(category, \"Investigate the issue further for a solution.\")\n",
        "\n",
        "def main():\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Fetching comments...\")\n",
        "    comments = get_comments(video_id)\n",
        "    if not comments:\n",
        "        print(\"No comments found.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Processing comments...\")\n",
        "    clean_comments = [clean_comment(c) for c in comments]\n",
        "    issue_comments = identify_issue_comments(clean_comments)\n",
        "\n",
        "    if not issue_comments:\n",
        "        print(\"‚úÖ No issue-related negative comments found.\")\n",
        "        return\n",
        "\n",
        "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
        "\n",
        "    print(\"\\nüö® Identified Issues and Recommendations üö®\")\n",
        "    for category, comments in categorized_issues.items():\n",
        "        print(f\"\\nüî∏ **{category} ({len(comments)} comments)**\")\n",
        "        for comment in comments[:5]:  # Display a few sample comments\n",
        "            print(f\"  - {comment}\")\n",
        "        print(f\"  ‚û§ Recommended Fix: {recommend_fixes(category)}\")\n",
        "\n",
        "        if category in timestamps_summary and timestamps_summary[category]:\n",
        "            print(f\"  ‚è∞ Issue Found At: {', '.join(set(timestamps_summary[category]))}\")\n",
        "\n",
        "    print(\"\\nüìå **Final Summary of All Issues with Timestamps** üìå\")\n",
        "    final_timestamps = {cat: list(set(timestamps)) for cat, timestamps in timestamps_summary.items() if timestamps}\n",
        "\n",
        "    if final_timestamps:\n",
        "        for category, times in final_timestamps.items():\n",
        "            print(f\"üîπ {category}: {', '.join(times)}\")\n",
        "    else:\n",
        "        print(\"‚úÖ No specific timestamps mentioned for detected issues.\")\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis Complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT8ITuWVObHp",
        "outputId": "c1303aa1-d2a0-4b68-d7fa-85c431abd693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://youtu.be/PqVvrF7pwDQ?si=zs1RM2myiy57KszD\n",
            "üîπ Fetching comments...\n",
            "üîπ Processing comments...\n",
            "\n",
            "üö® Identified Issues and Recommendations üö®\n",
            "\n",
            "üî∏ **Other Issues (60 comments)**\n",
            "  - samsung s25 edge apu nisa apple lase4 eka 16e kara venna bada\n",
            "  - i thought its bad but now addicted\n",
            "  - tamil nadu badshah\n",
            "  - bad song\n",
            "  - honestly speaking this song gives me a bad headache\n",
            "  ‚û§ Recommended Fix: Investigate the mentioned issues and consider appropriate fixes.\n",
            "  ‚è∞ Issue Found At: 331, 224, 230, 347, 127, 330, 336, 0330\n",
            "\n",
            "üî∏ **Framing Issues (1 comments)**\n",
            "  - 039 laye framing issue iruku nenaikrennnn\n",
            "  ‚û§ Recommended Fix: Adjust camera positioning to maintain proper framing and avoid unnecessary empty space.\n",
            "  ‚è∞ Issue Found At: 039\n",
            "\n",
            "üî∏ **Video Issues (11 comments)**\n",
            "  - ive watched this song so many times that i couldnt help but notice the changes not sure if im the first one to point it out but heres what i observed\n",
            "\n",
            "1 the entire video has been reuploadedwonder how many of you noticed\n",
            "\n",
            "2 the image of meenakshi at 029 has been replaced\n",
            "\n",
            "3 some shake effects have been added here and there though im not entirely sure\n",
            "\n",
            "4 i noticed some effect changes at 257 again not completely sure\n",
            "\n",
            "5 the main reason for the reupload seems to be the transition at 330 where abhay walks into the light in the earlier version there was an editors error where a timestamp from the footage appeared for a few milliseconds during the transition this has been corrected in the new version\n",
            "\n",
            "6 a significant change at 404 in the previous version sai placed his hands on meenakshis face before the cut in this edit meenakshi disintegrates into dust before transitioning to the next scene\n",
            "\n",
            "that being said hats off to sai abhyankkar for delivering an absolute banger despite these minor edits this song is truly a masterpiece fan of your work da thambi\n",
            "  - helo there is error in the footage at 331 the end the shot footage there was an video file name 41220fbmxf in the left bottom corner  in the right bottom corner src tc 12\n",
            "  - please focus on music rather your dance after you two blockbuster music video this video is not really expected from you lyrics and the quality of the music is really bad i think you really focus on your music lyrics again i know you can do much better than this because your first two songs are really good so please dont make this type of songs\n",
            "  - came to see the video glitch at 33105 sec\n",
            "  - 329 how many of you are here after the insta video for editing mistake\n",
            "  ‚û§ Recommended Fix: Review video resolution, color grading, and any potential artifacts or glitches.\n",
            "  ‚è∞ Issue Found At: 029h m s, 331, 257, 404, 330, 329, 329h m s, 029, 33105\n",
            "\n",
            "üî∏ **Audio Issues (1 comments)**\n",
            "  - us senate candidates we need action\n",
            "two us senate candidates and two state representative candidates shared similar views its actually astounding and infuriating that these types of egregious human rights problems are going on in china and that we are aware of it in our country and that we are not addressing the issues said independent senate nominee and former state senator dr mike katz but we fail because of the influence of money its very concerning and we need strong leadership in this country to step forward and to deal with it immediately\n",
            "\n",
            "\n",
            "independent senate nominee and former state senator dr mike katz\n",
            "\n",
            "senator katz emphasized how the film revealed the true nature of communism and its harm on humanity its been a slow infiltration of communism mindset in this country its occurring in our academic institutions its the influence of money coming in from overseas that is teaching our students not how to critically think but what to think and that is very concerning about the future of our country but its been going on for several years he said\n",
            "\n",
            "senator katz said he has always been aware that there were human rights problems in china and he called on people to become aware of the issues and not just stand by\n",
            "\n",
            "us senate candidate eric hansen said he was shocked when he heard about the forced organ harvesting in china which is supported by the ccp its just such a tragedy and the fact that the world doesnt know about this the fact that youre drawing attention to it is very important he said\n",
            "\n",
            "\n",
            "us senate candidate eric hansen right with falun gong practitioner cheng peiming\n",
            "\n",
            "mr hansen said the abhorrent crime of organ harvesting must be ended get the word out please because its such a huge issue thats being unaddressed he stated\n",
            "  ‚û§ Recommended Fix: Check the audio levels, remove background noise, and verify microphone quality.\n",
            "\n",
            "üìå **Final Summary of All Issues with Timestamps** üìå\n",
            "üîπ Other Issues: 331, 224, 230, 347, 127, 330, 336, 0330\n",
            "üîπ Framing Issues: 039\n",
            "üîπ Video Issues: 029h m s, 331, 257, 404, 330, 329, 329h m s, 029, 33105\n",
            "\n",
            "‚úÖ Analysis Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TO GIVE OVERALL STATS AND SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "0Ogn0n9hgYn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from googleapiclient.discovery import build\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize YouTube API\n",
        "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
        "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "\n",
        "# Initialize sentiment analysis model\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "\n",
        "def extract_video_id(url):\n",
        "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
        "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "\n",
        "def get_video_statistics(video_id):\n",
        "    \"\"\"Fetch video statistics like title, views, likes, and comments count.\"\"\"\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet,statistics\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    if not response[\"items\"]:\n",
        "        return None\n",
        "\n",
        "    video_data = response[\"items\"][0]\n",
        "    stats = {\n",
        "        \"title\": video_data[\"snippet\"][\"title\"],\n",
        "        \"channel\": video_data[\"snippet\"][\"channelTitle\"],\n",
        "        \"published_date\": video_data[\"snippet\"][\"publishedAt\"][:10],  # Extract YYYY-MM-DD\n",
        "        \"views\": int(video_data[\"statistics\"].get(\"viewCount\", 0)),\n",
        "        \"likes\": int(video_data[\"statistics\"].get(\"likeCount\", 0)),\n",
        "        \"comments\": int(video_data[\"statistics\"].get(\"commentCount\", 0))\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "\n",
        "def get_comments(video_id):\n",
        "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "    while request:\n",
        "        response = request.execute()\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "        request = youtube.commentThreads().list_next(request, response)\n",
        "    return comments\n",
        "\n",
        "\n",
        "def detect_language(comment):\n",
        "    \"\"\"Detect the language of a comment.\"\"\"\n",
        "    try:\n",
        "        return detect(comment)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def clean_comment(comment):\n",
        "    \"\"\"Clean the comment while keeping timestamps (do NOT remove colons).\"\"\"\n",
        "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
        "    return comment.strip().lower()  # Convert to lowercase, but keep timestamps\n",
        "\n",
        "\n",
        "def analyze_sentiment(comment):\n",
        "    \"\"\"Analyze the sentiment of a comment, truncating if necessary.\"\"\"\n",
        "    MAX_LENGTH = 512  # Maximum token length for the model\n",
        "    truncated_comment = comment[:MAX_LENGTH]\n",
        "    result = sentiment_pipeline(truncated_comment)[0]\n",
        "    return result[\"label\"], result[\"score\"]\n",
        "\n",
        "\n",
        "def extract_timestamps(comment):\n",
        "    \"\"\"Extract timestamps from a comment in the format mm:ss or hh:mm:ss.\"\"\"\n",
        "    raw_timestamps = re.findall(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\", comment)\n",
        "    formatted_timestamps = set(raw_timestamps)  # Use a set to remove duplicates\n",
        "    return formatted_timestamps\n",
        "\n",
        "\n",
        "def identify_issue_comments(comments):\n",
        "    \"\"\"Identify comments that mention issues and have negative sentiment, along with timestamps.\"\"\"\n",
        "    issue_keywords = [\n",
        "        \"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\",\n",
        "        \"not working\", \"bad\", \"broken\", \"off\", \"lag\", \"delay\", \"freeze\", \"stutter\",\n",
        "        \"audio\", \"video\", \"sync\", \"framing\"\n",
        "    ]\n",
        "    issue_comments = []\n",
        "    sentiment_scores = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n",
        "\n",
        "    for comment in comments:\n",
        "        sentiment, score = analyze_sentiment(comment)\n",
        "\n",
        "        if sentiment == \"POSITIVE\":\n",
        "            sentiment_scores[\"positive\"] += 1\n",
        "        elif sentiment == \"NEGATIVE\":\n",
        "            sentiment_scores[\"negative\"] += 1\n",
        "        else:\n",
        "            sentiment_scores[\"neutral\"] += 1\n",
        "\n",
        "        if any(keyword in comment for keyword in issue_keywords) or extract_timestamps(comment):\n",
        "            if sentiment == \"NEGATIVE\" and score > 0.60:\n",
        "                issue_comments.append(comment)\n",
        "\n",
        "    return issue_comments, sentiment_scores\n",
        "\n",
        "\n",
        "def categorize_issues(issue_comments):\n",
        "    \"\"\"Categorize detected issues and store timestamps properly.\"\"\"\n",
        "    categorized_issues = defaultdict(list)\n",
        "    timestamps_summary = defaultdict(set)\n",
        "\n",
        "    for comment in issue_comments:\n",
        "        timestamps = extract_timestamps(comment)\n",
        "\n",
        "        if any(word in comment for word in [\"audio\", \"sound\", \"voice\", \"mic\"]):\n",
        "            categorized_issues[\"Audio Issues\"].append(comment)\n",
        "            timestamps_summary[\"Audio Issues\"].update(timestamps)\n",
        "\n",
        "        elif any(word in comment for word in [\"video\", \"visual\", \"quality\", \"blurry\", \"pixelated\"]):\n",
        "            categorized_issues[\"Video Issues\"].append(comment)\n",
        "            timestamps_summary[\"Video Issues\"].update(timestamps)\n",
        "\n",
        "        elif any(word in comment for word in [\"sync\", \"delay\", \"out of sync\", \"lag\"]):\n",
        "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
        "            timestamps_summary[\"Synchronization Issues\"].update(timestamps)\n",
        "\n",
        "        elif any(word in comment for word in [\"framing\", \"cropped\", \"off-center\"]):\n",
        "            categorized_issues[\"Framing Issues\"].append(comment)\n",
        "            timestamps_summary[\"Framing Issues\"].update(timestamps)\n",
        "\n",
        "        else:\n",
        "            categorized_issues[\"Other Issues\"].append(comment)\n",
        "            timestamps_summary[\"Other Issues\"].update(timestamps)\n",
        "\n",
        "    return categorized_issues, timestamps_summary\n",
        "\n",
        "\n",
        "def main():\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL.\")\n",
        "        return\n",
        "\n",
        "    # Fetch Video Statistics\n",
        "    stats = get_video_statistics(video_id)\n",
        "    if not stats:\n",
        "        print(\"Failed to retrieve video data.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüìä **Video Statistics**\")\n",
        "    print(f\"üé• Title: {stats['title']}\")\n",
        "    print(f\"üë§ Channel: {stats['channel']}\")\n",
        "    print(f\"üìÖ Published Date: {stats['published_date']}\")\n",
        "    print(f\"üëÄ Views: {stats['views']:,}\")\n",
        "    print(f\"üëç Likes: {stats['likes']:,}\")\n",
        "    print(f\"üí¨ Comments: {stats['comments']:,}\")\n",
        "\n",
        "    # Fetch Comments and Analyze\n",
        "    print(\"\\nüîπ Fetching comments...\")\n",
        "    comments = get_comments(video_id)\n",
        "    if not comments:\n",
        "        print(\"No comments found.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Processing comments...\")\n",
        "    clean_comments = [clean_comment(c) for c in comments]\n",
        "    issue_comments, sentiment_scores = identify_issue_comments(clean_comments)\n",
        "\n",
        "    total_comments = sum(sentiment_scores.values())\n",
        "    if total_comments > 0:\n",
        "        positivity = (sentiment_scores[\"positive\"] / total_comments) * 100\n",
        "        negativity = (sentiment_scores[\"negative\"] / total_comments) * 100\n",
        "        print(\"\\nüì¢ **Viewer Sentiment Summary**\")\n",
        "        print(f\"üü¢ Positive: {positivity:.2f}% | üî¥ Negative: {negativity:.2f}% | ‚ö™ Neutral: {100 - positivity - negativity:.2f}%\")\n",
        "\n",
        "    # Categorize Issues\n",
        "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
        "    # (Displays categorized issues and timestamps as in the previous version...)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9imv4AZUg7F",
        "outputId": "a5947b04-8b71-48c7-c38b-9dabdcae9fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://youtu.be/PqVvrF7pwDQ?si=zs1RM2myiy57KszD\n",
            "\n",
            "üìä **Video Statistics**\n",
            "üé• Title: Sai Abhyankkar - Sithira Puthiri (Music Video) | Meenakshi Chaudhary | Akshay Sundher | Think Indie\n",
            "üë§ Channel: Think Music India\n",
            "üìÖ Published Date: 2025-01-31\n",
            "üëÄ Views: 24,093,941\n",
            "üëç Likes: 306,319\n",
            "üí¨ Comments: 9,254\n",
            "\n",
            "üîπ Fetching comments...\n",
            "üîπ Processing comments...\n",
            "\n",
            "üì¢ **Viewer Sentiment Summary**\n",
            "üü¢ Positive: 49.03% | üî¥ Negative: 50.97% | ‚ö™ Neutral: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORKS WITH TIMESTAMPS"
      ],
      "metadata": {
        "id": "-ZUECkLMTFrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from googleapiclient.discovery import build\n",
        "from langdetect import detect\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize YouTube API\n",
        "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
        "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
        "\n",
        "# Initialize sentiment analysis model\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "\n",
        "def extract_video_id(url):\n",
        "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
        "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "\n",
        "def get_comments(video_id):\n",
        "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
        "    comments = []\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,\n",
        "        textFormat=\"plainText\"\n",
        "    )\n",
        "    while request:\n",
        "        response = request.execute()\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(comment)\n",
        "        request = youtube.commentThreads().list_next(request, response)\n",
        "    return comments\n",
        "\n",
        "\n",
        "def detect_language(comment):\n",
        "    \"\"\"Detect the language of a comment.\"\"\"\n",
        "    try:\n",
        "        return detect(comment)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def clean_comment(comment):\n",
        "    \"\"\"Clean the comment while keeping timestamps (do NOT remove colons).\"\"\"\n",
        "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
        "    return comment.strip().lower()  # Convert to lowercase, but keep timestamps\n",
        "\n",
        "\n",
        "def analyze_sentiment(comment):\n",
        "    \"\"\"Analyze the sentiment of a comment, truncating if necessary.\"\"\"\n",
        "    MAX_LENGTH = 512  # Maximum token length for the model\n",
        "\n",
        "    # Truncate the comment if it's too long\n",
        "    truncated_comment = comment[:MAX_LENGTH]\n",
        "\n",
        "    result = sentiment_pipeline(truncated_comment)[0]\n",
        "    return result[\"label\"], result[\"score\"]\n",
        "\n",
        "\n",
        "def extract_timestamps(comment):\n",
        "    \"\"\"Extract timestamps from a comment in the format mm:ss or hh:mm:ss.\"\"\"\n",
        "    raw_timestamps = re.findall(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\", comment)\n",
        "    formatted_timestamps = set(raw_timestamps)  # Use a set to remove duplicates\n",
        "    return formatted_timestamps\n",
        "\n",
        "\n",
        "def identify_issue_comments(comments):\n",
        "    \"\"\"Identify comments that mention issues and have negative sentiment, along with timestamps.\"\"\"\n",
        "    issue_keywords = [\n",
        "        \"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\",\n",
        "        \"not working\", \"bad\", \"broken\", \"off\", \"lag\", \"delay\", \"freeze\", \"stutter\",\n",
        "        \"audio\", \"video\", \"sync\", \"framing\"\n",
        "    ]\n",
        "    issue_comments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        if any(keyword in comment for keyword in issue_keywords) or extract_timestamps(comment):\n",
        "            sentiment, score = analyze_sentiment(comment)\n",
        "            if sentiment == \"NEGATIVE\" and score > 0.60:  # Moderate to strong negative sentiment\n",
        "                issue_comments.append(comment)\n",
        "\n",
        "    return issue_comments\n",
        "\n",
        "\n",
        "def categorize_issues(issue_comments):\n",
        "    \"\"\"Categorize detected issues and store timestamps properly.\"\"\"\n",
        "    categorized_issues = defaultdict(list)\n",
        "    timestamps_summary = defaultdict(set)  # Using a set to avoid duplicate timestamps\n",
        "\n",
        "    for comment in issue_comments:\n",
        "        timestamps = extract_timestamps(comment)  # Extract timestamps\n",
        "\n",
        "        if any(word in comment.lower() for word in [\"audio\", \"sound\", \"voice\", \"mic\"]):\n",
        "            categorized_issues[\"Audio Issues\"].append(comment)\n",
        "            timestamps_summary[\"Audio Issues\"].update(timestamps)\n",
        "\n",
        "        elif any(word in comment.lower() for word in [\"video\", \"visual\", \"quality\", \"blurry\", \"pixelated\"]):\n",
        "            categorized_issues[\"Video Issues\"].append(comment)\n",
        "            timestamps_summary[\"Video Issues\"].update(timestamps)\n",
        "\n",
        "        elif any(word in comment.lower() for word in [\"sync\", \"delay\", \"out of sync\", \"lag\"]):\n",
        "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
        "            timestamps_summary[\"Synchronization Issues\"].update(timestamps)\n",
        "\n",
        "        elif any(word in comment.lower() for word in [\"framing\", \"cropped\", \"off-center\", \"too much space\"]):\n",
        "            categorized_issues[\"Framing Issues\"].append(comment)\n",
        "            timestamps_summary[\"Framing Issues\"].update(timestamps)\n",
        "\n",
        "        else:\n",
        "            categorized_issues[\"Other Issues\"].append(comment)\n",
        "            timestamps_summary[\"Other Issues\"].update(timestamps)\n",
        "\n",
        "    return categorized_issues, timestamps_summary\n",
        "\n",
        "\n",
        "def recommend_fixes(category):\n",
        "    \"\"\"Provide recommendations based on issue categories.\"\"\"\n",
        "    fixes = {\n",
        "        \"Audio Issues\": \"Check the audio levels, remove background noise, and verify microphone quality.\",\n",
        "        \"Video Issues\": \"Review video resolution, color grading, and any potential artifacts or glitches.\",\n",
        "        \"Synchronization Issues\": \"Ensure proper audio-video sync during editing and encoding.\",\n",
        "        \"Framing Issues\": \"Adjust camera positioning to maintain proper framing and avoid unnecessary empty space.\",\n",
        "        \"Other Issues\": \"Investigate the mentioned issues and consider appropriate fixes.\"\n",
        "    }\n",
        "    return fixes.get(category, \"Investigate the issue further for a solution.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Fetching comments...\")\n",
        "    comments = get_comments(video_id)\n",
        "    if not comments:\n",
        "        print(\"No comments found.\")\n",
        "        return\n",
        "\n",
        "    print(\"üîπ Processing comments...\")\n",
        "    clean_comments = [clean_comment(c) for c in comments]\n",
        "    issue_comments = identify_issue_comments(clean_comments)\n",
        "\n",
        "    if not issue_comments:\n",
        "        print(\"‚úÖ No issue-related negative comments found.\")\n",
        "        return\n",
        "\n",
        "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
        "\n",
        "    print(\"\\nüö® Identified Issues and Recommendations üö®\")\n",
        "    for category, comments in categorized_issues.items():\n",
        "        print(f\"\\nüî∏ **{category} ({len(comments)} comments)**\")\n",
        "        for comment in comments[:5]:  # Display a few sample comments\n",
        "            print(f\"  - {comment}\")\n",
        "        print(f\"  ‚û§ Recommended Fix: {recommend_fixes(category)}\")\n",
        "\n",
        "        if timestamps_summary[category]:\n",
        "            formatted_timestamps = ', '.join(sorted(timestamps_summary[category]))\n",
        "            print(f\"  ‚è∞ Issue Found At: {formatted_timestamps}\")\n",
        "\n",
        "    print(\"\\nüìå **Final Summary of All Issues with Timestamps** üìå\")\n",
        "    for category, timestamps in timestamps_summary.items():\n",
        "        if timestamps:\n",
        "            formatted_timestamps = ', '.join(sorted(timestamps))\n",
        "            print(f\"üîπ {category}: {formatted_timestamps}\")\n",
        "    if not any(timestamps_summary.values()):\n",
        "        print(\"‚úÖ No specific timestamps mentioned for detected issues.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq85IDy-QRMg",
        "outputId": "e57c35c0-b805-48bb-9f60-c1849ed86863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://youtu.be/PqVvrF7pwDQ?si=zs1RM2myiy57KszD\n",
            "üîπ Fetching comments...\n",
            "üîπ Processing comments...\n",
            "\n",
            "üö® Identified Issues and Recommendations üö®\n",
            "\n",
            "üî∏ **Other Issues (344 comments)**\n",
            "  - 2:57 is a goosbumps..................‚ù§‚ù§‚ù§‚ù§\n",
            "  - 2:22 drop\n",
            "  - 2:27 who is she\n",
            "  - first time found on insta reel 2:21 my fav part ...\n",
            "  - 1:54 my fav partü´∂üèª\n",
            "  ‚û§ Recommended Fix: Investigate the mentioned issues and consider appropriate fixes.\n",
            "  ‚è∞ Issue Found At: 00:41, 00:45, 01:27, 02:08, 02:55, 03:30, 03:37, 03:52, 0:01, 0:05, 0:06, 0:07, 0:08, 0:09, 0:10, 0:11, 0:15, 0:16, 0:17, 0:18, 0:23, 0:25, 0:26, 0:27, 0:28, 0:30, 0:31, 0:32, 0:33, 0:34, 0:35, 0:36, 0:37, 0:38, 0:39, 0:40, 0:41, 0:42, 0:44, 0:45, 0:47, 0:49, 0:52, 0:59, 11:02, 12:07, 1:00, 1:01, 1:03, 1:06, 1:07, 1:12, 1:13, 1:14, 1:22, 1:24, 1:27, 1:28, 1:30, 1:31, 1:33, 1:40, 1:41, 1:42, 1:43, 1:48, 1:49, 1:54, 1:55, 1:58, 1:59, 2:00, 2:05, 2:08, 2:09, 2:10, 2:11, 2:13, 2:15, 2:19, 2:20, 2:21, 2:22, 2:23, 2:24, 2:26, 2:27, 2:28, 2:29, 2:35, 2:36, 2:37, 2:38, 2:39, 2:40, 2:41, 2:43, 2:45, 2:46, 2:47, 2:48, 2:49, 2:50, 2:54, 2:55, 2:56, 2:57, 2:58, 2:59, 3:00, 3:02, 3:03, 3:09, 3:10, 3:11, 3:15, 3:16, 3:17, 3:19, 3:20, 3:22, 3:24, 3:25, 3:28, 3:29, 3:30, 3:31, 3:33, 3:35, 3:36, 3:37, 3:38, 3:39, 3:43, 3:46, 3:47, 3:48, 3:49, 3:50, 3:51, 3:52, 3:53, 3:54, 3:55, 3:56, 3:57, 3:58, 3:59, 4:00, 4:02, 4:03, 4:04, 4:05, 4:12, 4:16, 4:18, 4:20, 4:38, 4:48, 5:01\n",
            "\n",
            "üî∏ **Video Issues (77 comments)**\n",
            "  - overrated song everything is a mess except that illa illa part. they have tried the best with video\n",
            "  - mass video bro\n",
            "i‚Äôm your subscriber üòä\n",
            "  - what a video production ‚ù§‚ù§üòÆüòÆ\n",
            "  - me who is watching this video in hiroshima during the world war 2üòÇ\n",
            "  - mayru mathiri irukku, video coverage  illa\n",
            "  ‚û§ Recommended Fix: Review video resolution, color grading, and any potential artifacts or glitches.\n",
            "  ‚è∞ Issue Found At: 0:29, 1:28, 2:57, 3:29, 3:30, 3:31, 3:31:05, 4:04\n",
            "\n",
            "üî∏ **Synchronization Issues (5 comments)**\n",
            "  - samaj nei aaa raha hai but suunne k lia a6aa lag raha haiüòÇüòÇ\n",
            "  - yeppa dey yenna paatu da idhu...  andha ulagathuke poi vibe panna mari iruku\n",
            "  - meenakshi a innum alaga can be shown\n",
            "  - intha moonjuku epdi ithana alagana heroines set avuthuüò°üò≠ namme ivlo alaga irkom loosu ku mari irkan ivanuku epdi intha heroines ok solranga\n",
            "  - aye superu semma mass. i waited from 6 but worth the wait ,i wished i could comment first but due to the delay i was unable to üò¢\n",
            "  ‚û§ Recommended Fix: Ensure proper audio-video sync during editing and encoding.\n",
            "\n",
            "üî∏ **Audio Issues (17 comments)**\n",
            "  - 2:10 kondaadum..sounds like danush voice\n",
            "  - guys, hear me out.. alot of things have been changed in the mv:\n",
            "\n",
            "0:09 : cgi photo frame(was not there before)\n",
            "0:28 : this was not the initial pose given before, it was more of a peace sign like:‚úå\n",
            "0:41 : the transition sound effect has been removed(i really loved it before)\n",
            "0:45 : the air wave effect has been added now!\n",
            "2:56 : the air wave effect again....\n",
            "\n",
            "and a bunch of shaky effects for a lot of transitions:\n",
            "3:28,  3:32, 3:35 - 3:40... and..3:43, 3:51\n",
            "\n",
            "3:58 : picture shown for a little longer\n",
            "4:03 : she does not fade away into ashes.. in the original video...\n",
            "4:18 : cgi photo frame spin (again)\n",
            "\n",
            "\n",
            "i personally thought some were making the video worse, and some were awesome..!!!\n",
            "how did y'all feel.. after watching it..\n",
            "\n",
            "\n",
            "üëÜü§ì\n",
            "  - 1:27 sounds like ta takkara‚ù§‚ù§\n",
            "  - 2:00 nadula dhanush voice marri erukiku ? ü§î\n",
            "  - 2:36 his voice....‚ù§Ô∏è damn it's almost a peak üî•\n",
            "  ‚û§ Recommended Fix: Check the audio levels, remove background noise, and verify microphone quality.\n",
            "  ‚è∞ Issue Found At: 0:09, 0:28, 0:41, 0:42, 0:45, 1:13, 1:27, 1:40, 2:00, 2:09, 2:10, 2:36, 2:56, 3:28, 3:32, 3:33, 3:35, 3:40, 3:43, 3:48, 3:50, 3:51, 3:58, 4:03, 4:18\n",
            "\n",
            "üî∏ **Framing Issues (1 comments)**\n",
            "  - 0:39 laye framing issue iruku nenaikrennnn\n",
            "  ‚û§ Recommended Fix: Adjust camera positioning to maintain proper framing and avoid unnecessary empty space.\n",
            "  ‚è∞ Issue Found At: 0:39\n",
            "\n",
            "üìå **Final Summary of All Issues with Timestamps** üìå\n",
            "üîπ Other Issues: 00:41, 00:45, 01:27, 02:08, 02:55, 03:30, 03:37, 03:52, 0:01, 0:05, 0:06, 0:07, 0:08, 0:09, 0:10, 0:11, 0:15, 0:16, 0:17, 0:18, 0:23, 0:25, 0:26, 0:27, 0:28, 0:30, 0:31, 0:32, 0:33, 0:34, 0:35, 0:36, 0:37, 0:38, 0:39, 0:40, 0:41, 0:42, 0:44, 0:45, 0:47, 0:49, 0:52, 0:59, 11:02, 12:07, 1:00, 1:01, 1:03, 1:06, 1:07, 1:12, 1:13, 1:14, 1:22, 1:24, 1:27, 1:28, 1:30, 1:31, 1:33, 1:40, 1:41, 1:42, 1:43, 1:48, 1:49, 1:54, 1:55, 1:58, 1:59, 2:00, 2:05, 2:08, 2:09, 2:10, 2:11, 2:13, 2:15, 2:19, 2:20, 2:21, 2:22, 2:23, 2:24, 2:26, 2:27, 2:28, 2:29, 2:35, 2:36, 2:37, 2:38, 2:39, 2:40, 2:41, 2:43, 2:45, 2:46, 2:47, 2:48, 2:49, 2:50, 2:54, 2:55, 2:56, 2:57, 2:58, 2:59, 3:00, 3:02, 3:03, 3:09, 3:10, 3:11, 3:15, 3:16, 3:17, 3:19, 3:20, 3:22, 3:24, 3:25, 3:28, 3:29, 3:30, 3:31, 3:33, 3:35, 3:36, 3:37, 3:38, 3:39, 3:43, 3:46, 3:47, 3:48, 3:49, 3:50, 3:51, 3:52, 3:53, 3:54, 3:55, 3:56, 3:57, 3:58, 3:59, 4:00, 4:02, 4:03, 4:04, 4:05, 4:12, 4:16, 4:18, 4:20, 4:38, 4:48, 5:01\n",
            "üîπ Video Issues: 0:29, 1:28, 2:57, 3:29, 3:30, 3:31, 3:31:05, 4:04\n",
            "üîπ Audio Issues: 0:09, 0:28, 0:41, 0:42, 0:45, 1:13, 1:27, 1:40, 2:00, 2:09, 2:10, 2:36, 2:56, 3:28, 3:32, 3:33, 3:35, 3:40, 3:43, 3:48, 3:50, 3:51, 3:58, 4:03, 4:18\n",
            "üîπ Framing Issues: 0:39\n"
          ]
        }
      ]
    }
  ]
}