{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwNns9FzDV8r",
    "outputId": "2897cbf1-a2dd-405e-e983-f8c32a635e1a"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwqGWODUThme"
   },
   "source": [
    "# PRINTS ONLY COMMENT AND SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c61ebdeeb404464281dafe00dbb2d9b8",
      "16c80048e7b84d6fb5d6a1ad7a542221",
      "b1bb0254287144d996098bb8a0b39055",
      "21725b42d4094f95a53abf90706ebd0f",
      "2c9bbd6f4a0e4d92903c0eb45df3a4ef",
      "5611142dde904579a69b08fe635cc8d6",
      "7c6c6d9d4d2f4ceb86fa714e94370c39",
      "974ba8ee5a7f433b9a4ee5180cea0201",
      "0c05d3dcdb544c7f98d9aa76701c9100",
      "b78d2f4564f2400c8540b5b3bd79f59b",
      "765534b0671242ebb14f9cd11a1ee98a",
      "d719a2e1930e4e3eb04e8cabb19e66d5",
      "5ae6f262fe3941f0b6e61408fd416cd4",
      "0aae521063694c639b6c81e66247581c",
      "a9c7ac3ca4ae4029be7ba5c58a35bb65",
      "222a02bf45d342789d30d5d9184dd6d4",
      "825b866fbdde43b5a5d4a6310d74aa9f",
      "fffe72f12d36498cbbe3786a8c2beb24",
      "27f4e3a5a5934d6a94cbb86465211b5a",
      "62c15a87931745e6a9eb8927aed8dd35",
      "270577c2be4d48f980b40deef46a2537",
      "d6759fb28af04cc4b99a6ab10e798dab",
      "9670bf7c245241edb9dc0bcb0ac924a4",
      "25a5e40291614bbeb5a5c9f1274234c5",
      "91d47ddba4994cfa9d6734e88997c2cb",
      "46432018c7c94ce4a6b643793689c355",
      "d0c1565a151b4838bed321a8177c672e",
      "3258c2fc2f454c62bf9571a8dad4a716",
      "ac253f90b5314cba975b9e81ee8db5bf",
      "9e51067711264eb8a4fced64d0b78749",
      "8a8813761f544b1c9491b494865e0d5c",
      "1a1eeffbb75a4b658f8cbbb7763f9c0a",
      "5c2f9f44785a417697539e6ca3b471ea",
      "9ceb16e83f8e4d31b6f2058e73204f83",
      "cafceb93587e4946ba9624ac9c00b135",
      "2eb80e56f9cc49fa8a3b1df031d0de87",
      "eafddacb81714af0a81f1d2e22a1d812",
      "e45a6ffc4c454f2c926c4779b306d578",
      "20f2e5ee3b234c13a3c9924abf338ab0",
      "41a310138bb84c18bcaf94880dfec7b9",
      "74f03eff69764f19a7ac7d36ad05f1c0",
      "1175ac16a0b14d348587be1015f18c0c",
      "46681183fc2d473995bb2530f0b538bf",
      "b7c81a985ac4467394d0e33520bf9a82",
      "d51f6beae0e948218e0d3b3394623494",
      "8168e82e54624f718a5c7a11645b0eaa",
      "3412441d2c8642a4a1e1433593f0e139",
      "667cfabbd7c34247ac48f44afb9057de",
      "5c299f5da0644f259f234d120bae3453",
      "319d2f43116d404e8d39d934708d7645",
      "254d1b17889d4b7c981a825f9715bf51",
      "7c2a5e39232d4326bc3ddd7f447ecd6f",
      "5335136c2e72431ba24ca73bde8ba526",
      "585bec5fea394e5db0c1049a8d2a7fd7",
      "ae300cc7947a42dd98008ea22e70836b",
      "2c328af20eba4124bde084634c3cb6fe",
      "b7826286466248358c52fe09c44b7b9a",
      "c7c4aeb7caa3416c805f7bec957e90a3",
      "44fe45f7b49d446dbbdd1d3effa0cd70",
      "d9e67637d6434ae38e92642de74129e2",
      "713e8dbcea1e40199ebde7bc628ae68b",
      "f0ddd2dccd464f72a3de01c0b2972f2c",
      "52adbb9ff8404340932b0c20b5b3591a",
      "f27cffd82b914e4b9d800bb99ddd1ca1",
      "683c80aa5a324701822fb4a9bdf385e4",
      "e1fb01b629d742239926534ef7f92b2e",
      "cd228ea3c2c44443887834947724f233",
      "4aa580dcf6754f81a16f77f333eba9b7",
      "2406bbb7c63c4ad7aa4af35b293d2087",
      "c9c94fd723d44c688b0dba438a460a57",
      "9193a34fe1734603a5825f0349a7f989",
      "52cbbb0c245e4663b13a493454f7ff3a",
      "2fa9c6cd3df24369b0a637c298f15234",
      "0f67a14f6a3e4468b31e30c86e939df1",
      "ecb196d33f174996a741b09c91ac93b2",
      "05467f67d1104187b00e42c688b2d879",
      "97e5ab38ca994c3abea844789021277f",
      "43dd5a64e50547c59fe7628c533b1f92",
      "b5121d61693a423fb4cd6e97f9f33c89",
      "047c4d85f06e4013acd42a19716379af",
      "d643fa560370446bb7277729c6b752fc",
      "e28c79d324074eec8acb123612940f56",
      "d2b0f54008414621a771c869a5f07a8e",
      "2e876b37896e4c47aacbf954e0018e2c",
      "f3ab39e888804432a647bfa89af9894a",
      "660824ea3d2944eca6308515e724bf41",
      "1703d7cb2f164e618ab5116128277fb3",
      "2286e3cba5cb461c9d3749697eb038d3",
      "558e5eea46314777996392b047912817",
      "760a65c1791545879e6722ba43db157a",
      "0fc5d3bee73342cfae111b1e4c416a17",
      "bef3799053dd4f53a9857c4cbcdd867c",
      "d60aed81383e4887ac8ffb7ba285ba1b",
      "6a983c78749b407b9107fa13836d2818",
      "ff1f191e09da42b5b24fdd154bb5869f",
      "941e90bc261243ad8a5100aaca2cd884",
      "b16d12a95b0d40c98a53a23deff1f1e0",
      "7a443c38166c488688ab189a79707362",
      "97e0748205124fb58eec831894994055",
      "2c706910387b4b89b0c8d057040d5690",
      "9c5ecd4001cf41cfa3f9e584ea1b5620",
      "5cb9954b404d4536bceeef3a0a8f1b4f",
      "b0236c2234b846deacf933c5d675cd13",
      "fb011762ac60460f912c0066204d1456",
      "2ecf870ac4a044d2a91c55df66e65cca",
      "6a1194f691004d3e9142acb5d2c26321",
      "78da097fc4c343ab8ad447ac0121afe8",
      "787191bb73af49bb8a07621f8977cf20",
      "67a5f92bd6f846089a6776a4e0d53058",
      "f2e10da811704c6680f5d961bb68b5e3",
      "806ebff8a95e46258133ab019cd97fb7",
      "d752c9586e3b422ca0c1a288b5709dd5",
      "aeccdd14d4b44394ac30be14140f2417",
      "cc4c45d65dfc4b4cae69f19cd49bdeb2",
      "d16c66c72e51475fbb76b5c6d8b2f22b",
      "7d6dcb776bad47bea6c2a92c4e0baedf",
      "1bae59f02902473a839d9d6b943af32c",
      "101432388a924dbd993fd8db35a6e6dd",
      "b19ea5ec2f6443c7a1d64f05fbb83451",
      "08df0841810b497ba66d7c202d8ca303",
      "a986e1df688344d5a0b3bb626fb6234b",
      "43ab8d9dafd040fa9d48272fbce5d27a",
      "1d3c1477819c46e1b6d47a4e778cebd9",
      "d9ee20a20100429db6a29a0503f47915",
      "9ad97a24d47346b7bc787834265b8044",
      "67875d390bca4d8899edcf9239393892",
      "601156eee4f74b569268670a7d4eda1d",
      "1bfa6b552ba64f80898f5794d656b367",
      "b345b84ef53d43b3bbe38d8d2d3977a9",
      "58d2a605ef234f69804009f275f96516",
      "377d006f41024a779d55dccb1e2e6e99",
      "2e3ec7a60e2045cea2a892ae8f5f62eb",
      "4556b4c181294c34991ccbb7b52c45ee",
      "e67fa1c6bea542acb14f8583f2f32c65",
      "c120ccb272a04e52b8254d74404c2aac",
      "df3f911366c74e11b5a8f403e3f2d135",
      "2d6ef320ca734f598a15a7886818239c",
      "a144d90b94314625869ff44892f6b60e",
      "cc69f3c71ad443c1aad9b9d16b372e28",
      "4a79f681ee4b4b239d618c3fb20b6cab",
      "a0eb4d27073645b0aa54f3ee04625836",
      "17fa1bb829f34b9caf350916525511ad",
      "5c25bf8fa3cc4a68a8bb2c1160d71319",
      "5f3b7cb0be10493f90c675b95b2eafb5",
      "4192c37ba1314ed9aff9ab473847d0c0",
      "bf567f5de68c4de6b9687dc56843b1a2",
      "4175c4f354b6479d90c4d550f2d10f4c",
      "1a90bdec96cf417da89b889d5f4c113a",
      "b950c58b8dd24f6e9b7d7aa263d4920a",
      "d5953c73558b487aa8c388a5e5378491",
      "ee03c23597d640de8182807199d10dc3",
      "56ca4c9da1584e35a873cae02ce5d03f",
      "df6d9701a78544bdbb1ba9e8e06eb668",
      "daf802101d1d40eeab45b5e4ab666cd9",
      "280a1e817ae047998586657f97f51ecb",
      "43124260d1f2477e979b1f404f1684a2",
      "77a72b5a10504a06aeef7139fc5eec91",
      "3d8cdc04378b4307921ae8780c6c8cf0",
      "849d05194de24d5494984d6c42037f91",
      "9cbad7099943443f936deb40a336aab2",
      "0249bd13e2f24acba89960ac689d6974",
      "ddf71f2b950744d4b2fbd79c051c9cf4",
      "49bba91539374a2ba77f8dc43196c465",
      "749bd8dcc3d44dacabf2132941d256b7",
      "64d5c9bc630f4c2392cdabd486816df6"
     ]
    },
    "id": "eW7IQgkXHdZx",
    "outputId": "7b07c985-a5a5-4751-e98b-c924489031c1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Initialize models\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_video_id(url):\n",
    "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
    "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_comments(video_id):\n",
    "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "def detect_language(comment):\n",
    "    \"\"\"Detect the language of a comment.\"\"\"\n",
    "    try:\n",
    "        return detect(comment)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_comment(comment):\n",
    "    \"\"\"Clean the comment by removing special characters and links.\"\"\"\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
    "    comment = re.sub(r\"[^\\w\\s]\", \"\", comment)  # Remove special characters\n",
    "    return comment.strip().lower()\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    \"\"\"Analyze the sentiment of a comment.\"\"\"\n",
    "    result = sentiment_pipeline(comment)[0]\n",
    "    return result[\"label\"], result[\"score\"]\n",
    "\n",
    "def identify_issue_comments(comments):\n",
    "    \"\"\"Identify comments that mention issues.\"\"\"\n",
    "    issue_keywords = [\"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\"]\n",
    "    issue_comments = []\n",
    "    for comment in comments:\n",
    "        if any(keyword in comment for keyword in issue_keywords):\n",
    "            issue_comments.append(comment)\n",
    "    return issue_comments\n",
    "\n",
    "def extract_timestamps(comment):\n",
    "    \"\"\"Extract timestamps from a comment.\"\"\"\n",
    "    return re.findall(r\"\\b\\d{1,2}:\\d{2}\\b\", comment)\n",
    "\n",
    "def recommend_fixes(issue_comments):\n",
    "    \"\"\"Provide recommendations based on identified issues.\"\"\"\n",
    "    recommendations = []\n",
    "    for comment in issue_comments:\n",
    "        if \"audio\" in comment or \"sound\" in comment:\n",
    "            recommendations.append(\"Check the audio levels and ensure there are no issues with the sound quality.\")\n",
    "        elif \"video\" in comment or \"visual\" in comment:\n",
    "            recommendations.append(\"Review the video quality and ensure there are no visual glitches.\")\n",
    "        elif \"sync\" in comment:\n",
    "            recommendations.append(\"Ensure that the audio and video are properly synchronized.\")\n",
    "        else:\n",
    "            recommendations.append(\"Review the mentioned issue and consider appropriate fixes.\")\n",
    "    return recommendations\n",
    "\n",
    "def main():\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    print(\"Fetching comments...\")\n",
    "    comments = get_comments(video_id)\n",
    "    if not comments:\n",
    "        print(\"No comments found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Processing comments...\")\n",
    "    clean_comments = [clean_comment(c) for c in comments]\n",
    "    issue_comments = identify_issue_comments(clean_comments)\n",
    "\n",
    "    if not issue_comments:\n",
    "        print(\"No issue-related comments found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nIdentified Issues and Recommendations:\")\n",
    "    for comment in issue_comments:\n",
    "        timestamps = extract_timestamps(comment)\n",
    "        sentiment, score = analyze_sentiment(comment)\n",
    "        recommendation = recommend_fixes([comment])[0]\n",
    "        print(f\"\\nComment: {comment}\")\n",
    "        if timestamps:\n",
    "            print(f\"Timestamps: {', '.join(timestamps)}\")\n",
    "        print(f\"Sentiment: {sentiment} (Confidence: {score:.2f})\")\n",
    "        print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUMNV3ykTP2k"
   },
   "source": [
    "# NO SUMMARY AND NO TIMESTAMPS BUT WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymgNQoZ0JvxW",
    "outputId": "501f1167-901c-4f2c-d0ff-bacad7bfccaf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Initialize models\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def extract_video_id(url):\n",
    "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
    "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_comments(video_id):\n",
    "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "def detect_language(comment):\n",
    "    \"\"\"Detect the language of a comment.\"\"\"\n",
    "    try:\n",
    "        return detect(comment)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_comment(comment):\n",
    "    \"\"\"Clean the comment by removing special characters and links.\"\"\"\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
    "    comment = re.sub(r\"[^\\w\\s]\", \"\", comment)  # Remove special characters\n",
    "    return comment.strip().lower()\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    \"\"\"Analyze the sentiment of a comment.\"\"\"\n",
    "    result = sentiment_pipeline(comment)[0]\n",
    "    return result[\"label\"], result[\"score\"]\n",
    "\n",
    "def identify_issue_comments(comments):\n",
    "    \"\"\"Identify comments that mention issues and have negative sentiment.\"\"\"\n",
    "    issue_keywords = [\"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\", \"not working\", \"bad\", \"broken\"]\n",
    "    issue_comments = []\n",
    "\n",
    "    for comment in comments:\n",
    "        if any(keyword in comment for keyword in issue_keywords):\n",
    "            sentiment, score = analyze_sentiment(comment)\n",
    "            if sentiment == \"NEGATIVE\" and score > 0.75:  # Ensure it's a strong negative sentiment\n",
    "                issue_comments.append(comment)\n",
    "\n",
    "    return issue_comments\n",
    "\n",
    "def extract_timestamps(comment):\n",
    "    \"\"\"Extract timestamps from a comment.\"\"\"\n",
    "    return re.findall(r\"\\b\\d{1,2}:\\d{2}\\b\", comment)\n",
    "\n",
    "def categorize_issues(issue_comments):\n",
    "    \"\"\"Categorize detected issues and summarize timestamps.\"\"\"\n",
    "    categorized_issues = defaultdict(list)\n",
    "    timestamps_summary = defaultdict(list)\n",
    "\n",
    "    for comment in issue_comments:\n",
    "        timestamps = extract_timestamps(comment)\n",
    "\n",
    "        if \"audio\" in comment or \"sound\" in comment:\n",
    "            categorized_issues[\"Audio Issues\"].append(comment)\n",
    "            if timestamps:\n",
    "                timestamps_summary[\"Audio Issues\"].extend(timestamps)\n",
    "\n",
    "        elif \"video\" in comment or \"visual\" in comment or \"quality\" in comment:\n",
    "            categorized_issues[\"Video Issues\"].append(comment)\n",
    "            if timestamps:\n",
    "                timestamps_summary[\"Video Issues\"].extend(timestamps)\n",
    "\n",
    "        elif \"sync\" in comment or \"delay\" in comment:\n",
    "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
    "            if timestamps:\n",
    "                timestamps_summary[\"Synchronization Issues\"].extend(timestamps)\n",
    "\n",
    "        else:\n",
    "            categorized_issues[\"Other Issues\"].append(comment)\n",
    "            if timestamps:\n",
    "                timestamps_summary[\"Other Issues\"].extend(timestamps)\n",
    "\n",
    "    return categorized_issues, timestamps_summary\n",
    "\n",
    "def recommend_fixes(category):\n",
    "    \"\"\"Provide recommendations based on issue categories.\"\"\"\n",
    "    fixes = {\n",
    "        \"Audio Issues\": \"Check the audio levels, remove background noise, and verify microphone quality.\",\n",
    "        \"Video Issues\": \"Review video resolution, color grading, and any potential artifacts or glitches.\",\n",
    "        \"Synchronization Issues\": \"Ensure proper audio-video sync during editing and encoding.\",\n",
    "        \"Other Issues\": \"Review the mentioned issues and consider appropriate fixes.\"\n",
    "    }\n",
    "    return fixes.get(category, \"Investigate the issue further for a solution.\")\n",
    "\n",
    "def main():\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Fetching comments...\")\n",
    "    comments = get_comments(video_id)\n",
    "    if not comments:\n",
    "        print(\"No comments found.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Processing comments...\")\n",
    "    clean_comments = [clean_comment(c) for c in comments]\n",
    "    issue_comments = identify_issue_comments(clean_comments)\n",
    "\n",
    "    if not issue_comments:\n",
    "        print(\"‚úÖ No issue-related negative comments found.\")\n",
    "        return\n",
    "\n",
    "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
    "\n",
    "    print(\"\\nüö® Identified Issues and Recommendations üö®\")\n",
    "    for category, comments in categorized_issues.items():\n",
    "        print(f\"\\nüî∏ **{category} ({len(comments)} comments)**\")\n",
    "        for comment in comments[:5]:  # Display a few sample comments\n",
    "            print(f\"  - {comment}\")\n",
    "        print(f\"  ‚û§ Recommended Fix: {recommend_fixes(category)}\")\n",
    "\n",
    "        if category in timestamps_summary and timestamps_summary[category]:\n",
    "            print(f\"  ‚è∞ Issue Found At: {', '.join(set(timestamps_summary[category]))}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis Complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyPOGr3yQeXo"
   },
   "source": [
    "# WORKS WITH TIMESTAMP BUT NOT IN PROPER FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KT8ITuWVObHp",
    "outputId": "c1303aa1-d2a0-4b68-d7fa-85c431abd693"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Initialize sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def extract_video_id(url):\n",
    "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
    "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_comments(video_id):\n",
    "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "def detect_language(comment):\n",
    "    \"\"\"Detect the language of a comment.\"\"\"\n",
    "    try:\n",
    "        return detect(comment)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def clean_comment(comment):\n",
    "    \"\"\"Clean the comment by removing special characters and links.\"\"\"\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
    "    comment = re.sub(r\"[^\\w\\s]\", \"\", comment)  # Remove special characters\n",
    "    return comment.strip().lower()\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    \"\"\"Analyze the sentiment of a comment, truncating if necessary.\"\"\"\n",
    "    MAX_LENGTH = 512  # Maximum token length for the model\n",
    "\n",
    "    # Truncate the comment if it's too long\n",
    "    truncated_comment = comment[:MAX_LENGTH]\n",
    "\n",
    "    result = sentiment_pipeline(truncated_comment)[0]\n",
    "    return result[\"label\"], result[\"score\"]\n",
    "\n",
    "def extract_timestamps(comment):\n",
    "    \"\"\"Extract timestamps from a comment in various formats.\"\"\"\n",
    "    time_patterns = [\n",
    "        r\"\\b\\d{1,2}:\\d{2}(:\\d{2})?\\b\",  # MM:SS or H:MM:SS (e.g., 12:34, 1:23:45)\n",
    "        r\"\\b\\d{3,4}\\b\",  # MMSS or MSS (e.g., 1234 for 12:34, 234 for 2:34)\n",
    "        r\"\\b\\d{4,5}\\b\",  # HMMSS or HMSS (e.g., 12345 for 1:23:45, 1234 for 12:34)\n",
    "        r\"(\\d+)\\s*h\\s*(\\d+)?\\s*m?\\s*(\\d+)?\\s*s?\",  # 1h 23m 45s, 5m 10s, etc.\n",
    "    ]\n",
    "    timestamps = []\n",
    "\n",
    "    for pattern in time_patterns:\n",
    "        matches = re.findall(pattern, comment)\n",
    "        for match in matches:\n",
    "            if isinstance(match, tuple):\n",
    "                hours, minutes, seconds = match\n",
    "                time_str = f\"{hours}h {minutes}m {seconds}s\".strip()\n",
    "            else:\n",
    "                time_str = match\n",
    "            timestamps.append(time_str)\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "def identify_issue_comments(comments):\n",
    "    \"\"\"Identify comments that mention issues and have negative sentiment.\"\"\"\n",
    "    issue_keywords = [\n",
    "        \"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\",\n",
    "        \"not working\", \"bad\", \"broken\", \"off\"\n",
    "    ]\n",
    "    issue_comments = []\n",
    "\n",
    "    for comment in comments:\n",
    "        if any(keyword in comment for keyword in issue_keywords):\n",
    "            sentiment, score = analyze_sentiment(comment)\n",
    "            if sentiment == \"NEGATIVE\" and score > 0.75:  # Strong negative sentiment\n",
    "                issue_comments.append(comment)\n",
    "\n",
    "    return issue_comments\n",
    "\n",
    "def categorize_issues(issue_comments):\n",
    "    \"\"\"Categorize detected issues and store timestamps properly.\"\"\"\n",
    "    categorized_issues = defaultdict(list)\n",
    "    timestamps_summary = defaultdict(list)\n",
    "\n",
    "    for comment in issue_comments:\n",
    "        timestamps = extract_timestamps(comment)  # Extract timestamps\n",
    "\n",
    "        if any(word in comment.lower() for word in [\"audio\", \"sound\", \"voice\", \"mic\"]):\n",
    "            categorized_issues[\"Audio Issues\"].append(comment)\n",
    "            timestamps_summary[\"Audio Issues\"].extend(timestamps)\n",
    "\n",
    "        elif any(word in comment.lower() for word in [\"video\", \"visual\", \"quality\", \"blurry\", \"pixelated\"]):\n",
    "            categorized_issues[\"Video Issues\"].append(comment)\n",
    "            timestamps_summary[\"Video Issues\"].extend(timestamps)\n",
    "\n",
    "        elif any(word in comment.lower() for word in [\"sync\", \"delay\", \"out of sync\", \"lag\"]):\n",
    "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
    "            timestamps_summary[\"Synchronization Issues\"].extend(timestamps)\n",
    "\n",
    "        elif any(word in comment.lower() for word in [\"framing\", \"cropped\", \"off-center\", \"too much space\"]):\n",
    "            categorized_issues[\"Framing Issues\"].append(comment)\n",
    "            timestamps_summary[\"Framing Issues\"].extend(timestamps)\n",
    "\n",
    "        else:\n",
    "            categorized_issues[\"Other Issues\"].append(comment)\n",
    "            timestamps_summary[\"Other Issues\"].extend(timestamps)\n",
    "\n",
    "    return categorized_issues, timestamps_summary\n",
    "\n",
    "def recommend_fixes(category):\n",
    "    \"\"\"Provide recommendations based on issue categories.\"\"\"\n",
    "    fixes = {\n",
    "        \"Audio Issues\": \"Check the audio levels, remove background noise, and verify microphone quality.\",\n",
    "        \"Video Issues\": \"Review video resolution, color grading, and any potential artifacts or glitches.\",\n",
    "        \"Synchronization Issues\": \"Ensure proper audio-video sync during editing and encoding.\",\n",
    "        \"Framing Issues\": \"Adjust camera positioning to maintain proper framing and avoid unnecessary empty space.\",\n",
    "        \"Other Issues\": \"Investigate the mentioned issues and consider appropriate fixes.\"\n",
    "    }\n",
    "    return fixes.get(category, \"Investigate the issue further for a solution.\")\n",
    "\n",
    "def main():\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Fetching comments...\")\n",
    "    comments = get_comments(video_id)\n",
    "    if not comments:\n",
    "        print(\"No comments found.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Processing comments...\")\n",
    "    clean_comments = [clean_comment(c) for c in comments]\n",
    "    issue_comments = identify_issue_comments(clean_comments)\n",
    "\n",
    "    if not issue_comments:\n",
    "        print(\"‚úÖ No issue-related negative comments found.\")\n",
    "        return\n",
    "\n",
    "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
    "\n",
    "    print(\"\\nüö® Identified Issues and Recommendations üö®\")\n",
    "    for category, comments in categorized_issues.items():\n",
    "        print(f\"\\nüî∏ **{category} ({len(comments)} comments)**\")\n",
    "        for comment in comments[:5]:  # Display a few sample comments\n",
    "            print(f\"  - {comment}\")\n",
    "        print(f\"  ‚û§ Recommended Fix: {recommend_fixes(category)}\")\n",
    "\n",
    "        if category in timestamps_summary and timestamps_summary[category]:\n",
    "            print(f\"  ‚è∞ Issue Found At: {', '.join(set(timestamps_summary[category]))}\")\n",
    "\n",
    "    print(\"\\nüìå **Final Summary of All Issues with Timestamps** üìå\")\n",
    "    final_timestamps = {cat: list(set(timestamps)) for cat, timestamps in timestamps_summary.items() if timestamps}\n",
    "\n",
    "    if final_timestamps:\n",
    "        for category, times in final_timestamps.items():\n",
    "            print(f\"üîπ {category}: {', '.join(times)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No specific timestamps mentioned for detected issues.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Analysis Complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ogn0n9hgYn4"
   },
   "source": [
    "# TO GIVE OVERALL STATS AND SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9imv4AZUg7F",
    "outputId": "a5947b04-8b71-48c7-c38b-9dabdcae9fb9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Initialize sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "\n",
    "def extract_video_id(url):\n",
    "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
    "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def get_video_statistics(video_id):\n",
    "    \"\"\"Fetch video statistics like title, views, likes, and comments count.\"\"\"\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    if not response[\"items\"]:\n",
    "        return None\n",
    "\n",
    "    video_data = response[\"items\"][0]\n",
    "    stats = {\n",
    "        \"title\": video_data[\"snippet\"][\"title\"],\n",
    "        \"channel\": video_data[\"snippet\"][\"channelTitle\"],\n",
    "        \"published_date\": video_data[\"snippet\"][\"publishedAt\"][:10],  # Extract YYYY-MM-DD\n",
    "        \"views\": int(video_data[\"statistics\"].get(\"viewCount\", 0)),\n",
    "        \"likes\": int(video_data[\"statistics\"].get(\"likeCount\", 0)),\n",
    "        \"comments\": int(video_data[\"statistics\"].get(\"commentCount\", 0))\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_comments(video_id):\n",
    "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "\n",
    "def detect_language(comment):\n",
    "    \"\"\"Detect the language of a comment.\"\"\"\n",
    "    try:\n",
    "        return detect(comment)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def clean_comment(comment):\n",
    "    \"\"\"Clean the comment while keeping timestamps (do NOT remove colons).\"\"\"\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
    "    return comment.strip().lower()  # Convert to lowercase, but keep timestamps\n",
    "\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    \"\"\"Analyze the sentiment of a comment, truncating if necessary.\"\"\"\n",
    "    MAX_LENGTH = 512  # Maximum token length for the model\n",
    "    truncated_comment = comment[:MAX_LENGTH]\n",
    "    result = sentiment_pipeline(truncated_comment)[0]\n",
    "    return result[\"label\"], result[\"score\"]\n",
    "\n",
    "\n",
    "def extract_timestamps(comment):\n",
    "    \"\"\"Extract timestamps from a comment in the format mm:ss or hh:mm:ss.\"\"\"\n",
    "    raw_timestamps = re.findall(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\", comment)\n",
    "    formatted_timestamps = set(raw_timestamps)  # Use a set to remove duplicates\n",
    "    return formatted_timestamps\n",
    "\n",
    "\n",
    "def identify_issue_comments(comments):\n",
    "    \"\"\"Identify comments that mention issues and have negative sentiment, along with timestamps.\"\"\"\n",
    "    issue_keywords = [\n",
    "        \"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\",\n",
    "        \"not working\", \"bad\", \"broken\", \"off\", \"lag\", \"delay\", \"freeze\", \"stutter\",\n",
    "        \"audio\", \"video\", \"sync\", \"framing\"\n",
    "    ]\n",
    "    issue_comments = []\n",
    "    sentiment_scores = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n",
    "\n",
    "    for comment in comments:\n",
    "        sentiment, score = analyze_sentiment(comment)\n",
    "\n",
    "        if sentiment == \"POSITIVE\":\n",
    "            sentiment_scores[\"positive\"] += 1\n",
    "        elif sentiment == \"NEGATIVE\":\n",
    "            sentiment_scores[\"negative\"] += 1\n",
    "        else:\n",
    "            sentiment_scores[\"neutral\"] += 1\n",
    "\n",
    "        if any(keyword in comment for keyword in issue_keywords) or extract_timestamps(comment):\n",
    "            if sentiment == \"NEGATIVE\" and score > 0.60:\n",
    "                issue_comments.append(comment)\n",
    "\n",
    "    return issue_comments, sentiment_scores\n",
    "\n",
    "\n",
    "def categorize_issues(issue_comments):\n",
    "    \"\"\"Categorize detected issues and store timestamps properly.\"\"\"\n",
    "    categorized_issues = defaultdict(list)\n",
    "    timestamps_summary = defaultdict(set)\n",
    "\n",
    "    for comment in issue_comments:\n",
    "        timestamps = extract_timestamps(comment)\n",
    "\n",
    "        if any(word in comment for word in [\"audio\", \"sound\", \"voice\", \"mic\"]):\n",
    "            categorized_issues[\"Audio Issues\"].append(comment)\n",
    "            timestamps_summary[\"Audio Issues\"].update(timestamps)\n",
    "\n",
    "        elif any(word in comment for word in [\"video\", \"visual\", \"quality\", \"blurry\", \"pixelated\"]):\n",
    "            categorized_issues[\"Video Issues\"].append(comment)\n",
    "            timestamps_summary[\"Video Issues\"].update(timestamps)\n",
    "\n",
    "        elif any(word in comment for word in [\"sync\", \"delay\", \"out of sync\", \"lag\"]):\n",
    "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
    "            timestamps_summary[\"Synchronization Issues\"].update(timestamps)\n",
    "\n",
    "        elif any(word in comment for word in [\"framing\", \"cropped\", \"off-center\"]):\n",
    "            categorized_issues[\"Framing Issues\"].append(comment)\n",
    "            timestamps_summary[\"Framing Issues\"].update(timestamps)\n",
    "\n",
    "        else:\n",
    "            categorized_issues[\"Other Issues\"].append(comment)\n",
    "            timestamps_summary[\"Other Issues\"].update(timestamps)\n",
    "\n",
    "    return categorized_issues, timestamps_summary\n",
    "\n",
    "\n",
    "def main():\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    # Fetch Video Statistics\n",
    "    stats = get_video_statistics(video_id)\n",
    "    if not stats:\n",
    "        print(\"Failed to retrieve video data.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nüìä **Video Statistics**\")\n",
    "    print(f\"üé• Title: {stats['title']}\")\n",
    "    print(f\"üë§ Channel: {stats['channel']}\")\n",
    "    print(f\"üìÖ Published Date: {stats['published_date']}\")\n",
    "    print(f\"üëÄ Views: {stats['views']:,}\")\n",
    "    print(f\"üëç Likes: {stats['likes']:,}\")\n",
    "    print(f\"üí¨ Comments: {stats['comments']:,}\")\n",
    "\n",
    "    # Fetch Comments and Analyze\n",
    "    print(\"\\nüîπ Fetching comments...\")\n",
    "    comments = get_comments(video_id)\n",
    "    if not comments:\n",
    "        print(\"No comments found.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Processing comments...\")\n",
    "    clean_comments = [clean_comment(c) for c in comments]\n",
    "    issue_comments, sentiment_scores = identify_issue_comments(clean_comments)\n",
    "\n",
    "    total_comments = sum(sentiment_scores.values())\n",
    "    if total_comments > 0:\n",
    "        positivity = (sentiment_scores[\"positive\"] / total_comments) * 100\n",
    "        negativity = (sentiment_scores[\"negative\"] / total_comments) * 100\n",
    "        print(\"\\nüì¢ **Viewer Sentiment Summary**\")\n",
    "        print(f\"üü¢ Positive: {positivity:.2f}% | üî¥ Negative: {negativity:.2f}% | ‚ö™ Neutral: {100 - positivity - negativity:.2f}%\")\n",
    "\n",
    "    # Categorize Issues\n",
    "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
    "    # (Displays categorized issues and timestamps as in the previous version...)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZUECkLMTFrP"
   },
   "source": [
    "# WORKS WITH TIMESTAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rq85IDy-QRMg",
    "outputId": "e57c35c0-b805-48bb-9f60-c1849ed86863"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyDCQYezL348y6KNmWKJSt_tgVYPeVep8hU\"  # Replace with your YouTube Data API v3 key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Initialize sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "\n",
    "def extract_video_id(url):\n",
    "    \"\"\"Extract YouTube video ID from a given URL.\"\"\"\n",
    "    pattern = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\\\"&?\\/\\s]{11})\"\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def get_comments(video_id):\n",
    "    \"\"\"Fetch comments from a YouTube video.\"\"\"\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "    return comments\n",
    "\n",
    "\n",
    "def detect_language(comment):\n",
    "    \"\"\"Detect the language of a comment.\"\"\"\n",
    "    try:\n",
    "        return detect(comment)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def clean_comment(comment):\n",
    "    \"\"\"Clean the comment while keeping timestamps (do NOT remove colons).\"\"\"\n",
    "    comment = re.sub(r\"http\\S+\", \"\", comment)  # Remove links\n",
    "    return comment.strip().lower()  # Convert to lowercase, but keep timestamps\n",
    "\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    \"\"\"Analyze the sentiment of a comment, truncating if necessary.\"\"\"\n",
    "    MAX_LENGTH = 512  # Maximum token length for the model\n",
    "\n",
    "    # Truncate the comment if it's too long\n",
    "    truncated_comment = comment[:MAX_LENGTH]\n",
    "\n",
    "    result = sentiment_pipeline(truncated_comment)[0]\n",
    "    return result[\"label\"], result[\"score\"]\n",
    "\n",
    "\n",
    "def extract_timestamps(comment):\n",
    "    \"\"\"Extract timestamps from a comment in the format mm:ss or hh:mm:ss.\"\"\"\n",
    "    raw_timestamps = re.findall(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\", comment)\n",
    "    formatted_timestamps = set(raw_timestamps)  # Use a set to remove duplicates\n",
    "    return formatted_timestamps\n",
    "\n",
    "\n",
    "def identify_issue_comments(comments):\n",
    "    \"\"\"Identify comments that mention issues and have negative sentiment, along with timestamps.\"\"\"\n",
    "    issue_keywords = [\n",
    "        \"issue\", \"problem\", \"error\", \"bug\", \"glitch\", \"fault\", \"mistake\", \"trouble\",\n",
    "        \"not working\", \"bad\", \"broken\", \"off\", \"lag\", \"delay\", \"freeze\", \"stutter\",\n",
    "        \"audio\", \"video\", \"sync\", \"framing\"\n",
    "    ]\n",
    "    issue_comments = []\n",
    "\n",
    "    for comment in comments:\n",
    "        if any(keyword in comment for keyword in issue_keywords) or extract_timestamps(comment):\n",
    "            sentiment, score = analyze_sentiment(comment)\n",
    "            if sentiment == \"NEGATIVE\" and score > 0.60:  # Moderate to strong negative sentiment\n",
    "                issue_comments.append(comment)\n",
    "\n",
    "    return issue_comments\n",
    "\n",
    "\n",
    "def categorize_issues(issue_comments):\n",
    "    \"\"\"Categorize detected issues and store timestamps properly.\"\"\"\n",
    "    categorized_issues = defaultdict(list)\n",
    "    timestamps_summary = defaultdict(set)  # Using a set to avoid duplicate timestamps\n",
    "\n",
    "    for comment in issue_comments:\n",
    "        timestamps = extract_timestamps(comment)  # Extract timestamps\n",
    "\n",
    "        if any(word in comment.lower() for word in [\"audio\", \"sound\", \"voice\", \"mic\"]):\n",
    "            categorized_issues[\"Audio Issues\"].append(comment)\n",
    "            timestamps_summary[\"Audio Issues\"].update(timestamps)\n",
    "\n",
    "        elif any(word in comment.lower() for word in [\"video\", \"visual\", \"quality\", \"blurry\", \"pixelated\"]):\n",
    "            categorized_issues[\"Video Issues\"].append(comment)\n",
    "            timestamps_summary[\"Video Issues\"].update(timestamps)\n",
    "\n",
    "        elif any(word in comment.lower() for word in [\"sync\", \"delay\", \"out of sync\", \"lag\"]):\n",
    "            categorized_issues[\"Synchronization Issues\"].append(comment)\n",
    "            timestamps_summary[\"Synchronization Issues\"].update(timestamps)\n",
    "\n",
    "        elif any(word in comment.lower() for word in [\"framing\", \"cropped\", \"off-center\", \"too much space\"]):\n",
    "            categorized_issues[\"Framing Issues\"].append(comment)\n",
    "            timestamps_summary[\"Framing Issues\"].update(timestamps)\n",
    "\n",
    "        else:\n",
    "            categorized_issues[\"Other Issues\"].append(comment)\n",
    "            timestamps_summary[\"Other Issues\"].update(timestamps)\n",
    "\n",
    "    return categorized_issues, timestamps_summary\n",
    "\n",
    "\n",
    "def recommend_fixes(category):\n",
    "    \"\"\"Provide recommendations based on issue categories.\"\"\"\n",
    "    fixes = {\n",
    "        \"Audio Issues\": \"Check the audio levels, remove background noise, and verify microphone quality.\",\n",
    "        \"Video Issues\": \"Review video resolution, color grading, and any potential artifacts or glitches.\",\n",
    "        \"Synchronization Issues\": \"Ensure proper audio-video sync during editing and encoding.\",\n",
    "        \"Framing Issues\": \"Adjust camera positioning to maintain proper framing and avoid unnecessary empty space.\",\n",
    "        \"Other Issues\": \"Investigate the mentioned issues and consider appropriate fixes.\"\n",
    "    }\n",
    "    return fixes.get(category, \"Investigate the issue further for a solution.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    video_id = extract_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Fetching comments...\")\n",
    "    comments = get_comments(video_id)\n",
    "    if not comments:\n",
    "        print(\"No comments found.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîπ Processing comments...\")\n",
    "    clean_comments = [clean_comment(c) for c in comments]\n",
    "    issue_comments = identify_issue_comments(clean_comments)\n",
    "\n",
    "    if not issue_comments:\n",
    "        print(\"‚úÖ No issue-related negative comments found.\")\n",
    "        return\n",
    "\n",
    "    categorized_issues, timestamps_summary = categorize_issues(issue_comments)\n",
    "\n",
    "    print(\"\\nüö® Identified Issues and Recommendations üö®\")\n",
    "    for category, comments in categorized_issues.items():\n",
    "        print(f\"\\nüî∏ **{category} ({len(comments)} comments)**\")\n",
    "        for comment in comments[:5]:  # Display a few sample comments\n",
    "            print(f\"  - {comment}\")\n",
    "        print(f\"  ‚û§ Recommended Fix: {recommend_fixes(category)}\")\n",
    "\n",
    "        if timestamps_summary[category]:\n",
    "            formatted_timestamps = ', '.join(sorted(timestamps_summary[category]))\n",
    "            print(f\"  ‚è∞ Issue Found At: {formatted_timestamps}\")\n",
    "\n",
    "    print(\"\\nüìå **Final Summary of All Issues with Timestamps** üìå\")\n",
    "    for category, timestamps in timestamps_summary.items():\n",
    "        if timestamps:\n",
    "            formatted_timestamps = ', '.join(sorted(timestamps))\n",
    "            print(f\"üîπ {category}: {formatted_timestamps}\")\n",
    "    if not any(timestamps_summary.values()):\n",
    "        print(\"‚úÖ No specific timestamps mentioned for detected issues.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
